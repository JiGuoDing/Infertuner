2025-08-08 07:02:19,002 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
2025-08-08 07:02:19,002 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
2025-08-08 07:02:19,172 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address 'gpu02' (127.0.0.1) for communication.
2025-08-08 07:02:19,196 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 127.0.0.1:0, bind address localhost:0.
2025-08-08 07:02:19,531 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2025-08-08 07:02:19,551 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2025-08-08 07:02:19,551 INFO  akka.remote.Remoting                                         [] - Starting remoting
2025-08-08 07:02:19,688 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:24267]
2025-08-08 07:02:19,788 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@127.0.0.1:24267
2025-08-08 07:02:19,802 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Using working directory: WorkingDirectory(/tmp/tm_127.0.0.1:24267-74379f)
2025-08-08 07:02:19,809 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2025-08-08 07:02:19,812 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 127.0.0.1:0, bind address localhost:0.
2025-08-08 07:02:19,829 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2025-08-08 07:02:19,833 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2025-08-08 07:02:19,837 INFO  akka.remote.Remoting                                         [] - Starting remoting
2025-08-08 07:02:19,849 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@127.0.0.1:13199]
2025-08-08 07:02:19,860 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@127.0.0.1:13199
2025-08-08 07:02:19,871 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_127.0.0.1:24267-74379f .
2025-08-08 07:02:19,882 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/tm_127.0.0.1:24267-74379f/blobStorage
2025-08-08 07:02:19,885 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/tm_127.0.0.1:24267-74379f/blobStorage
2025-08-08 07:02:19,888 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
2025-08-08 07:02:19,888 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Loading delegation token receivers
2025-08-08 07:02:19,891 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hadoopfs loaded and initialized
2025-08-08 07:02:19,892 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receiver hbase loaded and initialized
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-prometheus
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-slf4j
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-influx
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-datadog
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: external-resource-gpu
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-graphite
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-statsd
2025-08-08 07:02:19,892 INFO  org.apache.flink.core.plugin.DefaultPluginManager            [] - Plugin loader with ID found, reusing it: metrics-jmx
2025-08-08 07:02:19,892 INFO  org.apache.flink.runtime.security.token.DelegationTokenReceiverRepository [] - Delegation token receivers loaded successfully
2025-08-08 07:02:19,892 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 127.0.0.1:24267-74379f
2025-08-08 07:02:19,904 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 1758 GB, usable 22 GB (1.25% usable)
2025-08-08 07:02:19,906 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	/tmp/flink-io-3d3ffcf7-ce4e-4029-921b-cd3792442926
2025-08-08 07:02:19,911 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 4 (manual), number of client threads: 4 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
2025-08-08 07:02:19,945 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	/tmp/flink-netty-shuffle-1892cf72-6954-4385-94fa-5821629b2b90
2025-08-08 07:02:20,080 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 343 MB for network buffer pool (number of memory segments: 10977, bytes per segment: 32768).
2025-08-08 07:02:20,087 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2025-08-08 07:02:20,117 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
2025-08-08 07:02:20,118 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 30 ms).
2025-08-08 07:02:20,120 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
2025-08-08 07:02:20,162 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 43 ms). Listening on SocketAddress /127.0.0.1:32275.
2025-08-08 07:02:20,163 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2025-08-08 07:02:20,177 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2025-08-08 07:02:20,187 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2025-08-08 07:02:20,189 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-f93e79b8-7bdc-46af-904c-703d03653942
2025-08-08 07:02:20,191 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@localhost:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
2025-08-08 07:02:20,383 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2025-08-08 07:02:20,480 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@localhost:6123/user/rpc/resourcemanager_* under registration id f61fee89257cd0df94e070a906be5b54.
2025-08-08 07:02:26,752 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 098b54832efa797287b1e67ca48c074e for job 8cdcbaa03c001de0ef964b3836c8f93f from resource manager with leader id 00000000000000000000000000000000.
2025-08-08 07:02:26,756 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 098b54832efa797287b1e67ca48c074e.
2025-08-08 07:02:26,757 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 8cdcbaa03c001de0ef964b3836c8f93f for job leader monitoring.
2025-08-08 07:02:26,758 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
2025-08-08 07:02:26,780 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2025-08-08 07:02:26,808 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 8cdcbaa03c001de0ef964b3836c8f93f.
2025-08-08 07:02:26,809 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 8cdcbaa03c001de0ef964b3836c8f93f.
2025-08-08 07:02:26,811 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 8cdcbaa03c001de0ef964b3836c8f93f.
2025-08-08 07:02:26,841 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 098b54832efa797287b1e67ca48c074e.
2025-08-08 07:02:26,856 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2025-08-08 07:02:26,862 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 8cdcbaa03c001de0ef964b3836c8f93f
2025-08-08 07:02:26,866 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 098b54832efa797287b1e67ca48c074e.
2025-08-08 07:02:26,867 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
2025-08-08 07:02:26,868 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 098b54832efa797287b1e67ca48c074e.
2025-08-08 07:02:26,871 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
2025-08-08 07:02:26,874 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading 8cdcbaa03c001de0ef964b3836c8f93f/p-0ae80e6e0030221e97063cd18d91085da40082a9-11c9c71702c046408b7a3458b9e731bf from localhost/127.0.0.1:13385
2025-08-08 07:02:26,933 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: org.apache.flink.runtime.state.hashmap.HashMapStateBackend@58e2e903
2025-08-08 07:02:26,934 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2a3b3a8a
2025-08-08 07:02:26,934 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2025-08-08 07:02:26,939 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@200dd9d5
2025-08-08 07:02:26,949 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
2025-08-08 07:02:27,040 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - 启动KV缓存推理服务 (策略=FLUID, 初始大小=5)
2025-08-08 07:02:32,140 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - KV缓存推理服务已启动
2025-08-08 07:02:32,143 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
2025-08-08 07:02:32,148 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 开始生成优化负载请求流，总数: 100
2025-08-08 07:02:34,685 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_000] 未命中: 2502.04ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:34,686 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #1 ===
2025-08-08 07:02:34,686 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_000 | 用户: user_003
2025-08-08 07:02:34,686 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:02:34,687 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，其基本原理包括以下几点：

1. 网络结构：深度学习使用多层神经网络（DNN）进行模型构建。每一层都会对输入数据进行一系列的非线性变...
2025-08-08 07:02:34,687 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 2502.0ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:02:34,687 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:34,687 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 高度集中访问 | 活跃用户: 3个 | 每用户session: 1 | 请求间隔: 150ms | 进度: 1/100
2025-08-08 07:02:36,946 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_001] 未命中: 2104.63ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:36,947 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #2 ===
2025-08-08 07:02:36,947 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_001 | 用户: user_001
2025-08-08 07:02:36,947 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:02:36,947 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种基于人工神经元模型的机器学习算法，它通过模拟人脑中神经元之间的信息传递过程，实现对复杂数据的学习和预测。以下是神经网络的工作原理：

1. 数据输...
2025-08-08 07:02:36,947 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 2104.6ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:02:36,948 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:38,065 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_002] 命中: 964.36ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:38,065 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #3 ===
2025-08-08 07:02:38,066 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_002 | 用户: user_001
2025-08-08 07:02:38,066 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:02:38,066 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它将已训练好的模型应用到新的任务上，以减少从头开始训练的时间和计算资源。在传统机器学...
2025-08-08 07:02:38,066 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 964.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:38,066 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:39,037 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_003] 命中: 818.78ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:39,038 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #4 ===
2025-08-08 07:02:39,038 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_003 | 用户: user_001
2025-08-08 07:02:39,038 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:02:39,038 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它允许将一个已训练的模型应用于新的任务或领域，而不必重新训练整个模型。简而言之，迁移...
2025-08-08 07:02:39,039 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 818.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:39,039 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:39,759 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_004] 命中: 567.13ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:39,759 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #5 ===
2025-08-08 07:02:39,759 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_004 | 用户: user_001
2025-08-08 07:02:39,759 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:02:39,760 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning）是一种机器学习技术，它将已训练好的模型应用于新的、具有不同特征的任务上。简单来说，迁移学习是指利用一个已经训练好的...
2025-08-08 07:02:39,760 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 567.1ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:39,760 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:02:39,760 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 5 | 成功: 5 (100.0%) | 平均耗时: 1391.4ms
2025-08-08 07:02:39,760 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:40,578 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_005] 命中: 665.59ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:40,578 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #6 ===
2025-08-08 07:02:40,579 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_005 | 用户: user_001
2025-08-08 07:02:40,579 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:02:40,579 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估一个机器学习模型的质量通常包括以下几个方面：

1. 准确性：准确性是衡量模型预测结果与真实值之间差异的度量。通常，使用交叉验证、混淆矩阵或准确率等方法来计...
2025-08-08 07:02:40,579 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 665.6ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:40,579 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:41,327 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_006] 命中: 595.32ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:41,327 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #7 ===
2025-08-08 07:02:41,327 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_006 | 用户: user_003
2025-08-08 07:02:41,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:02:41,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使用算法和模型来从数据中自动学习规律和模式，并从中提取有用的信息。它的主要目标是使计算机系统能够自主地改进性能、解决复杂的问题或任...
2025-08-08 07:02:41,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 595.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:41,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:42,200 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_007] 命中: 719.98ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:42,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #8 ===
2025-08-08 07:02:42,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_007 | 用户: user_003
2025-08-08 07:02:42,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:02:42,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它允许计算机系统从数据中自动提取模式和规律，并使用这些模式和规律来预测未来的结果或行为。它可以通过训练算法，让计算机系统通过大量的数...
2025-08-08 07:02:42,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 720.0ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:42,202 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:43,182 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_008] 命中: 828.07ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:43,182 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #9 ===
2025-08-08 07:02:43,183 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_008 | 用户: user_001
2025-08-08 07:02:43,183 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:43,183 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 为了优化模型性能，可以采用以下几种方法：

1. 数据预处理：对原始数据进行清洗、标准化和归一化等处理，以便于后续的特征工程和模型训练。例如，对于图像分类问题，...
2025-08-08 07:02:43,183 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 828.1ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:43,183 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:44,887 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_009] 未命中: 1551.0700000000002ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:44,888 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #10 ===
2025-08-08 07:02:44,888 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_009 | 用户: user_002
2025-08-08 07:02:44,888 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:44,888 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能通常涉及以下几个步骤：

1. **数据预处理**：在训练模型之前，需要对数据进行清洗、标准化和特征工程。这一步包括去除异常值、填充缺失值、转换为数...
2025-08-08 07:02:44,889 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1551.1ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:02:44,889 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:02:44,889 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 10 | 成功: 10 (100.0%) | 平均耗时: 1131.7ms
2025-08-08 07:02:44,889 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:45,934 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_010] 命中: 892.01ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:45,934 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #11 ===
2025-08-08 07:02:45,934 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_010 | 用户: user_003
2025-08-08 07:02:45,934 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:02:45,935 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习是一种机器学习技术，它通过在不同任务之间共享知识和经验来提高模型的性能。它将一个已训练好的模型应用到另一个不同的任务上，并利用先前学习到的知识和参数，以...
2025-08-08 07:02:45,935 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 892.0ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:45,935 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:46,841 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_011] 命中: 753.78ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #12 ===
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_011 | 用户: user_001
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种人工智能技术，它基于多层神经网络模型，通过模拟人脑的学习过程，从数据中提取特征并进行分类、预测或生成等任务。其基本原理主要包括以下几个方面：

1...
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 753.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:46,842 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:47,698 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_012] 命中: 703.56ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:47,698 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #13 ===
2025-08-08 07:02:47,698 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_012 | 用户: user_001
2025-08-08 07:02:47,698 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:02:47,698 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种模仿生物神经系统功能的计算模型，它通过大量训练数据和权重参数的学习过程来模拟人类大脑的工作方式。以下是神经网络工作的一些基本步骤：

1. 数据输...
2025-08-08 07:02:47,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 703.6ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:47,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:47,699 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 高度集中访问 | 活跃用户: 3个 | 每用户session: 1 | 请求间隔: 150ms | 进度: 13/100
2025-08-08 07:02:48,781 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_013] 命中: 929.91ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:48,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #14 ===
2025-08-08 07:02:48,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_013 | 用户: user_001
2025-08-08 07:02:48,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:02:48,782 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 过拟合是指在训练数据集上表现良好的模型，在新未见过的数据集上表现不佳的现象。过拟合通常发生在机器学习模型中，特别是在处理具有高维度和复杂性的问题时。当模型过于复...
2025-08-08 07:02:48,782 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 929.9ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:48,782 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:49,743 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_014] 命中: 808.98ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:49,743 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #15 ===
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_014 | 用户: user_001
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能可以通过以下几种方式实现：

1. 数据预处理：对训练数据进行清洗、去噪和归一化等操作，使得数据集更加规范化，有助于提高模型的泛化能力。

2. 特...
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 809.0ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 15 | 成功: 15 (100.0%) | 平均耗时: 1027.0ms
2025-08-08 07:02:49,744 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:50,698 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_015] 命中: 800.02ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #16 ===
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_015 | 用户: user_002
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 过拟合是指机器学习模型在训练数据上表现良好，但在测试数据上的表现却较差的现象。过拟合通常发生在机器学习算法中，特别是当模型的复杂度非常高或者训练数据集中的噪声过...
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 800.0ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:50,699 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:51,766 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_016] 命中: 915.27ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #17 ===
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_016 | 用户: user_003
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能的常用方法有以下几点：

1. 数据预处理：对原始数据进行清洗、归一化、标准化等操作，以便更好地适应模型训练。例如，对图像数据可以使用归一化或中心归...
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 915.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:51,767 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:52,782 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_017] 命中: 862.89ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #18 ===
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_017 | 用户: user_002
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 要优化模型性能，可以采取以下几种方法：

1. 数据预处理：在训练模型之前，需要对数据进行预处理，包括清洗、归一化、标准化等操作，以确保数据的一致性和准确性。此...
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 862.9ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:52,783 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:54,927 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_018] 未命中: 1991.1799999999998ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #19 ===
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_018 | 用户: user_004
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机视觉和自然语言处理技术，用于在输入数据中识别出特定的信息或对象，并将它们与后续的处理任务关联起来。这种机制通常由两个部分组成：全局注意力和...
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1991.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:02:54,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:55,083 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID调整检查: 当前速率=0.93, 历史均值=0.98, 扩容阈值=1.32, 缩容阈值=0.64, 当前缓存=5
2025-08-08 07:02:55,722 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_019] 命中: 638.14ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:55,723 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #20 ===
2025-08-08 07:02:55,723 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_019 | 用户: user_003
2025-08-08 07:02:55,723 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:02:55,723 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型的质量通常包括以下几个方面：

1. 准确率：这是衡量模型预测结果与真实值之间差异程度的一个重要指标。准确率越高，说明模型在处理数据时的准确性越高。

...
2025-08-08 07:02:55,723 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 638.1ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:55,724 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:02:55,724 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 20 | 成功: 20 (100.0%) | 平均耗时: 1030.6ms
2025-08-08 07:02:55,724 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:57,873 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_020] 未命中: 1996.26ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:57,873 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #21 ===
2025-08-08 07:02:57,873 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_020 | 用户: user_008
2025-08-08 07:02:57,873 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:02:57,873 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 为了优化模型性能，可以采取以下几种策略：

1. 数据增强：通过数据扩充（如旋转、翻转、缩放等）和添加噪声来增加训练集的多样性，提高模型在新数据上的泛化能力。
...
2025-08-08 07:02:57,874 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1996.3ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:02:57,874 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:58,800 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_021] 命中: 774.55ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #22 ===
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_021 | 用户: user_008
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机程序设计技术，用于处理和管理输入数据中的注意力信息。它可以帮助机器学习模型更好地理解和识别输入数据中的关键点或对象，并在处理这些数据时选择...
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 774.6ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:58,801 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:02:59,799 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_022] 命中: 845.57ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:02:59,799 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #23 ===
2025-08-08 07:02:59,799 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_022 | 用户: user_002
2025-08-08 07:02:59,799 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:02:59,800 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制（Attention Mechanism）是一种机器学习模型，用于处理和理解文本、图像和其他类型的数据。它通过计算输入数据中各个部分对整体目标信息的重...
2025-08-08 07:02:59,800 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 845.6ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:02:59,800 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:01,781 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_023] 未命中: 1827.69ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:01,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #24 ===
2025-08-08 07:03:01,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_023 | 用户: user_006
2025-08-08 07:03:01,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:03:01,781 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，其基本原理是通过构建多层神经网络模型来模拟人脑的学习过程。以下是一些深度学习的基本原理：

1. 层次化架构：深度学习通常由多个层次...
2025-08-08 07:03:01,782 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1827.7ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:01,782 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:02,503 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_024] 命中: 569.42ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:02,503 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #25 ===
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_024 | 用户: user_008
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种人工智能技术，它使用多层神经网络来模拟人类大脑的高级认知过程，从而实现复杂的模式识别、自然语言处理和计算机视觉任务。其基本原理包括以下几个方面：
...
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 569.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 25 | 成功: 25 (100.0%) | 平均耗时: 1065.0ms
2025-08-08 07:03:02,504 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:02,504 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 中等分散访问 | 活跃用户: 8个 | 每用户session: 2 | 请求间隔: 150ms | 进度: 25/100
2025-08-08 07:03:03,253 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_025] 命中: 596.88ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #26 ===
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_025 | 用户: user_006
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型（也称为深度学习语言模型或大型语言模型）是一种人工智能技术，它使用机器学习和深度神经网络来理解和生成自然语言文本。其主要目标是模仿人类的自然语言理解能...
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 596.9ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:03,254 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:04,324 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_026] 命中: 917.39ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:04,324 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #27 ===
2025-08-08 07:03:04,324 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_026 | 用户: user_003
2025-08-08 07:03:04,324 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:03:04,325 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning）是一种机器学习技术，它利用已经训练好的模型在新的任务上进行预测或分类。这种技术的主要思想是：将一个已有的机器学习模...
2025-08-08 07:03:04,325 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 917.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:04,325 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:06,022 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_027] 未命中: 1544.03ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #28 ===
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_027 | 用户: user_001
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它允许模型在新的领域中应用其在训练数据集上获得的特定知识和特征，而无需重新训练整个模...
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1544.0ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:06,022 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:07,720 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_028] 未命中: 1545.21ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #29 ===
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_028 | 用户: user_004
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能的分支，它使用算法和统计模型让计算机从数据中自动学习，并利用这种学习来解决实际问题。机器学习可以分为监督学习、无监督学习和强化学习三种类型...
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1545.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:07,721 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:08,581 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_029] 命中: 708.24ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #30 ===
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_029 | 用户: user_003
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种人工智能技术，它利用多层神经网络来模拟人脑的复杂思维过程。它的基本原理是通过训练数据集中的输入和输出特征，让模型能够自动提取特征并进行分类或回归任...
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 708.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 30 | 成功: 30 (100.0%) | 平均耗时: 1064.6ms
2025-08-08 07:03:08,582 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:09,549 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_030] 命中: 814.36ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:09,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #31 ===
2025-08-08 07:03:09,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_030 | 用户: user_008
2025-08-08 07:03:09,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:09,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制（Attention Mechanism）是一种用于机器学习和深度学习的神经网络技术，它能够有效地处理大量的输入数据，并在每个时间步上对这些数据进行局...
2025-08-08 07:03:09,550 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 814.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:09,550 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:10,264 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_031] 命中: 562.15ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:10,264 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #32 ===
2025-08-08 07:03:10,264 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_031 | 用户: user_008
2025-08-08 07:03:10,264 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:10,264 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机程序或算法，用于处理和理解输入数据中的一系列信息，并根据输入数据的特征（例如内容、形状、颜色等）确定哪些部分对当前任务具有最大的注意力。在...
2025-08-08 07:03:10,265 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 562.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:10,265 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:12,019 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_032] 未命中: 1602.02ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #33 ===
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_032 | 用户: user_005
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它让计算机系统可以从数据中自动提取规律和模式，并利用这些规律和模式来解决未知问题或完成任务。它的基本思想是通过构建数学模型，让计算机...
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1602.0ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:12,020 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:12,797 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_033] 命中: 624.74ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #34 ===
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_033 | 用户: user_008
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型（Large Language Model，简称LLM）是一种人工智能技术，它使用大量的文本数据和自然语言处理算法来训练一种能够理解和生成人类语言的机...
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 624.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:12,797 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:13,549 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_034] 命中: 599.75ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:13,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #35 ===
2025-08-08 07:03:13,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_034 | 用户: user_008
2025-08-08 07:03:13,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:03:13,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型的质量通常涉及到多个方面，包括以下几个关键因素：

1. 模型性能：模型的预测准确性、召回率、F1分数等指标是评估模型质量的重要指标。这些指标可以用来衡...
2025-08-08 07:03:13,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 599.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:13,550 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:13,550 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 35 | 成功: 35 (100.0%) | 平均耗时: 1032.6ms
2025-08-08 07:03:13,550 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:14,328 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_035] 命中: 626.18ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:14,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #36 ===
2025-08-08 07:03:14,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_035 | 用户: user_001
2025-08-08 07:03:14,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:03:14,328 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它利用已有的知识和经验，将特定领域的知识应用于其他领域的问题解决中。迁移学习的基本思...
2025-08-08 07:03:14,329 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 626.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:14,329 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:16,233 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_036] 未命中: 1752.21ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #37 ===
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_036 | 用户: user_025
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型（Deep Learning Model，简称DLM）是一种基于深度学习技术的计算机程序，用于生成人类可理解的语言文本或回答问题。它通常由大量的训练数...
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1752.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:16,234 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:16,234 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 高度分散访问 | 活跃用户: 25个 | 每用户session: 3 | 请求间隔: 150ms | 进度: 37/100
2025-08-08 07:03:18,096 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_037] 未命中: 1709.88ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #38 ===
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_037 | 用户: user_013
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 解释一下Transformer架构
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: Transformer是一种深度学习模型，由Google在2017年提出。它是一个用于自然语言处理（NLP）任务的神经网络结构，主要用于文本生成、机器翻译、问答...
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1709.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:18,097 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:19,867 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_038] 未命中: 1617.35ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:19,867 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #39 ===
2025-08-08 07:03:19,867 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_038 | 用户: user_018
2025-08-08 07:03:19,868 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:03:19,868 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型的质量通常涉及以下几个方面：

1. 准确性：这是最基本也是最重要的指标。准确率是指模型正确预测的样本数占总样本数的比例，它可以通过以下公式计算：准确率...
2025-08-08 07:03:19,868 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1617.4ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:19,868 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:20,019 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID调整检查: 当前速率=1.13, 历史均值=1.02, 扩容阈值=1.38, 缩容阈值=0.67, 当前缓存=5
2025-08-08 07:03:21,748 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_039] 未命中: 1727.2ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:21,749 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #40 ===
2025-08-08 07:03:21,749 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_039 | 用户: user_019
2025-08-08 07:03:21,749 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:21,749 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型，也称为深度学习模型或大规模语言模型，是一种使用深度神经网络技术的自然语言处理（NLP）模型。它可以在文本、语音和图像等多模态数据中自动识别、理解和生...
2025-08-08 07:03:21,749 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1727.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:21,750 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:21,750 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 40 | 成功: 40 (100.0%) | 平均耗时: 1089.3ms
2025-08-08 07:03:21,750 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:22,528 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_040] 命中: 625.68ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:22,528 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #41 ===
2025-08-08 07:03:22,529 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_040 | 用户: user_013
2025-08-08 07:03:22,529 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:03:22,529 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它利用已经训练好的模型或知识库，将它们从一个任务迁移到另一个任务上，以提高新任务的性...
2025-08-08 07:03:22,529 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 625.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:22,529 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:24,364 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_041] 未命中: 1683.25ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #42 ===
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_041 | 用户: user_004
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”（简称GPT-3）是一种由OpenAI开发的深度学习模型，它能够生成人类语言文本，具有广泛的应用场景和潜力。它是一种能够理解、生成、创作各种形式的...
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1683.3ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:24,365 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:25,284 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_042] 命中: 767.16ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:25,284 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #43 ===
2025-08-08 07:03:25,284 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_042 | 用户: user_019
2025-08-08 07:03:25,284 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:03:25,284 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 过拟合是指在训练模型时，其参数过度适应训练数据集中的噪声和异常值，导致模型在新的、未见过的数据上表现不佳的现象。简单来说，过拟合就是模型在训练过程中过于关注了训...
2025-08-08 07:03:25,285 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 767.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:25,286 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:27,018 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_043] 未命中: 1579.8600000000001ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #44 ===
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_043 | 用户: user_009
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使计算机系统能够从经验数据中自动学习和改进性能，而无需显式编程。它的基本原理是通过使用算法和统计模型，让计算机系统可以从数据中发现...
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1579.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:27,019 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:28,784 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_044] 未命中: 1612.35ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:28,784 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #45 ===
2025-08-08 07:03:28,784 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_044 | 用户: user_017
2025-08-08 07:03:28,784 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:03:28,784 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型质量通常包括以下步骤：

1. **数据预处理**：首先，需要对原始数据进行清洗、转换和标准化。这可能涉及到删除重复值、填充缺失值、归一化或标准化数值等...
2025-08-08 07:03:28,785 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1612.4ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:28,785 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:28,785 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 45 | 成功: 45 (100.0%) | 平均耗时: 1107.6ms
2025-08-08 07:03:28,785 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:30,865 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_045] 未命中: 1928.04ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #46 ===
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_045 | 用户: user_014
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种由大量的节点（称为神经元）组成，用于模拟人脑的神经元功能和学习过程的计算模型。神经网络的工作原理可以分为以下几个步骤：

1. 数据输入：神经网络...
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1928.0ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:30,866 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:32,855 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_046] 未命中: 1837.15ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #47 ===
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_046 | 用户: user_006
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”是指能够理解自然语言文本，并生成与之相关、连贯和准确的文本或回答的技术。这种技术通常基于深度学习和神经网络架构，这些架构可以从大量的文本数据中学习...
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1837.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:32,856 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:34,826 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_047] 未命中: 1818.24ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #48 ===
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_047 | 用户: user_016
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机视觉系统或机器学习模型，用于在处理图像、视频或其他媒体数据时，对信息进行选择和组织。它通过识别和跟踪特定的对象或特征，将注意力集中在特定区...
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1818.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:34,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:35,894 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_048] 命中: 915.44ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:35,894 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #49 ===
2025-08-08 07:03:35,894 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_048 | 用户: user_014
2025-08-08 07:03:35,894 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:35,894 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机视觉和自然语言处理技术，用于在多任务环境中跟踪和聚焦于目标对象。它通过分析输入图像中的关键点、边缘和纹理特征，以及文本内容的语义信息，来确...
2025-08-08 07:03:35,894 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 915.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:35,895 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:35,895 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 高度分散访问 | 活跃用户: 25个 | 每用户session: 3 | 请求间隔: 150ms | 进度: 49/100
2025-08-08 07:03:37,587 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_049] 未命中: 1539.9099999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:37,587 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #50 ===
2025-08-08 07:03:37,587 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_049 | 用户: user_018
2025-08-08 07:03:37,587 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:03:37,587 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习方法，其基本原理是通过构建多层神经网络来模仿人脑的神经系统工作模式。深度学习的核心思想是将数据输入到神经网络中，然后通过反向传播算法调整网...
2025-08-08 07:03:37,588 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1539.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:37,588 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:37,588 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 50 | 成功: 50 (100.0%) | 平均耗时: 1157.6ms
2025-08-08 07:03:37,588 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:39,726 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_050] 未命中: 1985.47ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #51 ===
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_050 | 用户: user_022
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 过拟合（Overfitting）是指在训练数据上，模型过度拟合了训练数据的细节特征，而无法很好地泛化到新的、未见过的数据集上的现象。简单来说，当一个模型过于关注...
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1985.5ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:39,726 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:41,738 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_051] 未命中: 1859.58ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #52 ===
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_051 | 用户: user_011
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 为了优化模型性能，可以采取以下步骤：

1. 数据预处理：首先需要对数据进行清洗和预处理，包括去除噪声、缺失值、异常值等，以便于后续的训练过程。此外，还可以对特...
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1859.6ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:41,739 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:43,830 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_052] 未命中: 1938.1599999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #53 ===
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_052 | 用户: user_019
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机视觉和机器学习算法，用于从图像中提取出具有特定特征的对象。它在图像分类、目标检测、物体识别、人脸识别等任务中起着关键作用。

注意力机制的...
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1938.2ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:43,830 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:44,823 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_053] 命中: 841.34ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #54 ===
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_053 | 用户: user_019
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 解释一下Transformer架构
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: Transformer是一种基于自注意力机制的深度学习模型，由Facebook在2017年提出。它是由三个主要组成部分组成：编码器、解码器和注意力机制。

1....
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 841.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:44,824 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:46,884 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_054] 未命中: 1907.74ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:46,884 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #55 ===
2025-08-08 07:03:46,884 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_054 | 用户: user_008
2025-08-08 07:03:46,884 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:03:46,884 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”是一种基于深度学习的自然语言处理技术，它使用大量的语料库和算法来模拟人类的语言理解和生成能力。它的主要目标是根据输入的文本或指令，自动从上下文中抽...
2025-08-08 07:03:46,885 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1907.7ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:46,885 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:46,885 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 55 | 成功: 55 (100.0%) | 平均耗时: 1207.5ms
2025-08-08 07:03:46,885 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:47,755 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_055] 命中: 718.54ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:47,755 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #56 ===
2025-08-08 07:03:47,755 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_055 | 用户: user_011
2025-08-08 07:03:47,755 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:03:47,755 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估一个机器学习模型的质量可以从多个角度进行，以下是一些主要的评估指标：

1. 准确率：准确率是最直接的评价指标之一，它表示模型正确预测的样本数量占总样本数的...
2025-08-08 07:03:47,756 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 718.5ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:03:47,756 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:49,863 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_056] 未命中: 2004.8899999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #57 ===
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_056 | 用户: user_012
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 解释一下Transformer架构
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: Transformer（Transformers）是一种深度学习模型，它由多个自注意力机制组成，主要用于处理序列数据。这种模型的主要特点包括：

1. 层次化：...
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 2004.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:49,863 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:51,690 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_057] 未命中: 1724.05ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #58 ===
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_057 | 用户: user_034
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机科学中的算法，用于处理和跟踪信息在输入中移动的过程。它通常涉及到将注意力从一个元素转移到另一个元素，以便更好地理解输入数据或任务的特征。
...
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1724.1ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:51,690 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:53,332 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_058] 未命中: 1538.9099999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #59 ===
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_058 | 用户: user_021
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 解释一下Transformer架构
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: Transformer（变换器）是一种深度学习模型，它是基于自注意力机制和编码器-解码器结构的。Transformer最初由Google在2017年提出，主要用...
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1538.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:53,332 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:53,433 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID调整检查: 当前速率=1.22, 历史均值=1.08, 扩容阈值=1.46, 缩容阈值=0.70, 当前缓存=5
2025-08-08 07:03:55,201 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_059] 未命中: 1765.63ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #60 ===
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_059 | 用户: user_003
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型质量通常涉及以下几个方面：

1. 准确率：准确率是衡量模型预测结果与真实值之间差异程度的指标，计算公式为：
   \( \text{Accuracy}...
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1765.6ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 60 | 成功: 60 (100.0%) | 平均耗时: 1236.1ms
2025-08-08 07:03:55,201 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:57,062 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_060] 未命中: 1758.38ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:57,062 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #61 ===
2025-08-08 07:03:57,062 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_060 | 用户: user_002
2025-08-08 07:03:57,062 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:03:57,062 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，其基本原理是通过多层神经网络模型，从数据中自动提取特征并构建出能够实现特定任务的模型。它主要由以下三个核心部分组成：

1. 输入层...
2025-08-08 07:03:57,063 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1758.4ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:57,063 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:03:57,063 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 混乱访问模式 | 活跃用户: 40个 | 每用户session: 随机1-5 | 请求间隔: 100ms | 进度: 61/100
2025-08-08 07:03:58,927 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_061] 未命中: 1762.55ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #62 ===
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_061 | 用户: user_026
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是机器学习和人工智能中的一种重要技术，它可以帮助计算机系统理解和处理复杂的数据集。在注意力机制中，一个计算机程序或模型会从输入数据集中选择并提取与给定...
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1762.6ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:03:58,928 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:00,637 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_062] 未命中: 1606.3ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #63 ===
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_062 | 用户: user_027
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种模仿人脑的计算模型，它通过一系列的节点（称为神经元）和连接来实现信息处理和学习。神经网络的工作原理可以分为以下步骤：

1. 输入数据输入：神经网...
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1606.3ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:00,637 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:02,363 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_063] 未命中: 1623.73ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #64 ===
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_063 | 用户: user_014
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 在机器学习和深度学习中，过拟合（Overfitting）是指模型在训练数据上表现很好，但在新数据上的泛化能力较差的现象。简单来说，过拟合就是模型过于复杂，以至于...
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1623.7ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:02,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:03,167 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_064] 命中: 701.29ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:03,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #65 ===
2025-08-08 07:04:03,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_064 | 用户: user_002
2025-08-08 07:04:03,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:04:03,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习是一种机器学习技术，它将已有的知识和经验应用于新的、未被训练的领域，以解决特定问题。在传统机器学习中，模型通常需要从一个领域（例如图像识别或自然语言处理...
2025-08-08 07:04:03,168 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 701.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:03,168 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:03,168 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 65 | 成功: 65 (100.0%) | 平均耗时: 1255.7ms
2025-08-08 07:04:03,168 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:04,792 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_065] 未命中: 1521.4099999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #66 ===
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_065 | 用户: user_022
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种基于人工神经元的机器学习模型，它通过模仿人脑神经元的工作原理来实现复杂的任务。神经网络由多个相互连接的节点组成，每个节点可以接收输入信号并产生输出...
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1521.4ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:04,792 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:06,417 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_066] 未命中: 1522.33ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #67 ===
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_066 | 用户: user_030
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种模仿人脑神经元结构和功能的计算模型，它由大量称为节点（或单元）的数字电路组成，这些节点可以接收输入信号并产生输出信号。每个节点都有一个权重，用于调...
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1522.3ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:06,417 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:08,227 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_067] 未命中: 1707.04ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #68 ===
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_067 | 用户: user_012
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使用数据和算法来让计算机系统能够自动从经验中学习和改进，从而实现智能化。它的目标是使计算机系统能够通过分析数据、模式识别、分类和预...
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1707.0ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:08,227 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:09,917 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_068] 未命中: 1587.5ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:09,917 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #69 ===
2025-08-08 07:04:09,917 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_068 | 用户: user_029
2025-08-08 07:04:09,917 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:04:09,917 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制是一种计算机程序或算法，用于处理和分析输入数据，以便将注意力集中在一个特定的对象或任务上。在机器学习、自然语言处理（NLP）、计算机视觉等人工智能领域...
2025-08-08 07:04:09,917 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1587.5ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:09,918 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:11,985 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_069] 未命中: 1965.92ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #70 ===
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_069 | 用户: user_013
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 要优化模型性能，可以采用以下几种方法：

1. 数据预处理：数据预处理是机器学习中的重要步骤。它包括清洗、转换和归一化等操作，以确保数据的质量和一致性。例如，可...
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1965.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 70 | 成功: 70 (100.0%) | 平均耗时: 1284.6ms
2025-08-08 07:04:11,986 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:13,716 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_070] 未命中: 1628.1100000000001ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #71 ===
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_070 | 用户: user_015
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使用统计模型和算法来从数据中自动发现模式、规律和趋势，并利用这些信息进行预测和决策。机器学习的目标是让计算机系统能够通过观察和学习...
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1628.1ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:13,717 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:15,527 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_071] 未命中: 1707.78ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #72 ===
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_071 | 用户: user_021
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能的方法有很多，以下是一些常用的技术：

1. **数据预处理**：在训练模型之前，需要对原始数据进行预处理，包括清洗、归一化、标准化等。这一步骤有助...
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1707.8ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:15,527 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:17,376 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_072] 未命中: 1746.5ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:17,376 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #73 ===
2025-08-08 07:04:17,376 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_072 | 用户: user_022
2025-08-08 07:04:17,376 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:17,377 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能通常包括以下步骤：

1. **数据预处理**：这是模型训练阶段的基础，需要对原始数据进行清洗、转换和格式化。这可能涉及去除噪声、填充缺失值、标准化...
2025-08-08 07:04:17,377 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1746.5ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:17,377 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:17,377 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 混乱访问模式 | 活跃用户: 40个 | 每用户session: 随机1-5 | 请求间隔: 100ms | 进度: 73/100
2025-08-08 07:04:19,237 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_073] 未命中: 1758.05ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #74 ===
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_073 | 用户: user_006
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”是指使用深度学习技术，能够模拟人类智能的自然语言处理系统。它是一种计算机程序，可以理解和生成人类语言，具备模仿人类思维、表达观点和解决问题的能力。...
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1758.1ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:19,237 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:21,142 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_074] 未命中: 1802.45ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:21,142 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #75 ===
2025-08-08 07:04:21,142 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_074 | 用户: user_030
2025-08-08 07:04:21,142 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:04:21,142 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使用统计模型和算法来让计算机系统从数据中自动学习规律，并从中提取出有用的特征和模式。换句话说，机器学习是通过给定大量数据集，训练计...
2025-08-08 07:04:21,143 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1802.5ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:21,143 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:21,143 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 75 | 成功: 75 (100.0%) | 平均耗时: 1314.2ms
2025-08-08 07:04:21,143 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:23,049 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_075] 未命中: 1803.94ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:23,049 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #76 ===
2025-08-08 07:04:23,049 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_075 | 用户: user_005
2025-08-08 07:04:23,049 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:04:23,049 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，它通过构建多层神经网络来实现对数据的自动特征提取和分类。它的基本原理如下：

1. 数据预处理：在训练深度学习模型之前，需要对输入数...
2025-08-08 07:04:23,050 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1803.9ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:23,050 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:25,166 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_076] 未命中: 2014.58ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #77 ===
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_076 | 用户: user_002
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种模仿人脑运作的计算机模型，它使用大量的训练数据来学习复杂的模式和规律。神经网络的工作原理如下：

1. 网络结构：神经网络通常由多个层次组成，每个...
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 2014.6ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:25,167 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:26,825 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_077] 未命中: 1556.49ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:26,825 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #78 ===
2025-08-08 07:04:26,825 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_077 | 用户: user_001
2025-08-08 07:04:26,825 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:04:26,825 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估一个机器学习模型的质量通常涉及以下几个步骤：

1. 数据集准备：首先，需要对数据进行预处理和清洗，包括去除噪声、填充缺失值、归一化等。确保数据集中包含足够...
2025-08-08 07:04:26,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1556.5ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:26,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:27,572 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_078] 命中: 644.77ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:27,572 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #79 ===
2025-08-08 07:04:27,572 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_078 | 用户: user_001
2025-08-08 07:04:27,573 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:04:27,573 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它允许将已训练的模型应用于新的任务或领域，而不需要重新从头开始构建新模型。它的基本思...
2025-08-08 07:04:27,573 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 644.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:27,573 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:27,674 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID调整检查: 当前速率=1.25, 历史均值=1.13, 扩容阈值=1.53, 缩容阈值=0.74, 当前缓存=5
2025-08-08 07:04:28,647 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_079] 命中: 971.68ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:28,647 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #80 ===
2025-08-08 07:04:28,647 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_079 | 用户: user_005
2025-08-08 07:04:28,647 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:04:28,647 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，它使用多层神经网络（MLP）来模拟人类大脑的高级认知过程。它的基本原理可以分为以下几步：

1. 数据预处理：首先，数据需要进行清洗...
2025-08-08 07:04:28,647 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 971.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:28,648 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:28,648 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 80 | 成功: 80 (100.0%) | 平均耗时: 1319.4ms
2025-08-08 07:04:28,648 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:29,797 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_080] 命中: 1047.82ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #81 ===
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_080 | 用户: user_002
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型，也称为深度学习语言模型，是一种基于深度神经网络的自然语言处理（NLP）技术。它使用大量文本数据和大量的标注信息来训练模型，以便在给定输入的情况下能够...
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1047.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:29,798 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:30,788 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_081] 命中: 888.41ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #82 ===
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_081 | 用户: user_001
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 解释一下Transformer架构
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: Transformer是一种深度学习模型，由Google在2017年提出。它最初用于自然语言处理任务，如机器翻译、问答系统和文本摘要等，但现在已被广泛应用于多个...
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 888.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:30,789 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:31,844 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_082] 命中: 953.21ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #83 ===
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_082 | 用户: user_002
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”是一种计算机程序，它能够理解和生成人类自然语言文本，例如回答问题、提供建议、创作故事等。这种技术可以基于深度学习和自然语言处理（NLP）算法进行训...
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 953.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:31,844 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:33,605 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_083] 未命中: 1658.1399999999999ms (+1000ms) (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #84 ===
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_083 | 用户: user_003
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能通常涉及以下几个步骤：

1. 数据预处理：首先，需要对训练数据进行清洗和预处理，包括去除无效数据、填充缺失值、归一化或标准化等。这一步可以提高模型...
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1658.1ms | 模型: Qwen1.5-1.8B-Chat-Miss
2025-08-08 07:04:33,605 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:34,645 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_084] 命中: 937.49ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:34,645 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #85 ===
2025-08-08 07:04:34,645 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_084 | 用户: user_001
2025-08-08 07:04:34,645 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:34,645 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”是一种人工智能技术，它使用深度学习和自然语言处理（NLP）方法来理解和生成人类语言。这种技术可以用于多种任务，包括文本生成、对话系统、机器翻译、问...
2025-08-08 07:04:34,645 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 937.5ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:34,646 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:34,646 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 85 | 成功: 85 (100.0%) | 平均耗时: 1306.4ms
2025-08-08 07:04:34,646 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:34,646 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 回归集中访问 | 活跃用户: 5个 | 每用户session: 1 | 请求间隔: 100ms | 进度: 85/100
2025-08-08 07:04:35,548 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_085] 命中: 800.8ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #86 ===
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_085 | 用户: user_001
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制（Attention Mechanism）是一种计算机科学和人工智能技术，用于处理和理解复杂的数据流。它主要用于机器学习、深度学习和其他需要大量计算资...
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 800.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:35,549 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:36,517 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_086] 命中: 865.47ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #87 ===
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_086 | 用户: user_003
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能的方法有很多，下面是一些常见的方法：

1. 数据预处理：数据预处理是机器学习中的重要步骤，包括清洗、归一化、标准化和特征选择等。通过这些步骤，可以...
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 865.5ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:36,517 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:37,509 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_087] 命中: 889.79ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #88 ===
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_087 | 用户: user_003
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型质量可以从多个方面进行，以下是一些常见的方法：

1. 准确率和精度：这是最常见的评估指标，它衡量的是模型预测结果与真实标签之间的相似程度。如果模型在训...
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 889.8ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:37,509 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:38,185 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_088] 命中: 574.24ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:38,185 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #89 ===
2025-08-08 07:04:38,185 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_088 | 用户: user_003
2025-08-08 07:04:38,185 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是过拟合？
2025-08-08 07:04:38,185 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 过拟合（Overfitting）是指在训练机器学习模型时，模型过于复杂，以至于过度拟合了训练数据中的噪声和随机扰动，而无法泛化到新的、未见过的数据上。换句话说，...
2025-08-08 07:04:38,186 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 574.2ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:38,186 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:38,826 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_089] 命中: 538.65ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #90 ===
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_089 | 用户: user_001
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是迁移学习？
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 迁移学习（Transfer Learning，简称TL）是一种机器学习技术，它将一个模型的结构和特征提取能力迁移到另一个任务上，而不必重新训练整个模型。这种技术...
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 538.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:38,826 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:38,827 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 90 | 成功: 90 (100.0%) | 平均耗时: 1274.6ms
2025-08-08 07:04:38,827 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:39,518 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_090] 命中: 589.65ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #91 ===
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_090 | 用户: user_003
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 优化模型性能的方法有很多，以下是一些常见的方法：

1. 数据预处理：对数据进行清洗、标准化和特征选择等操作，以提高模型的准确性。例如，去除异常值、填充缺失值、...
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 589.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:39,518 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:40,317 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_091] 命中: 697.3ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:40,317 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #92 ===
2025-08-08 07:04:40,317 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_091 | 用户: user_002
2025-08-08 07:04:40,317 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何评估模型质量？
2025-08-08 07:04:40,317 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 评估模型质量通常包括以下几个步骤：

1. **数据预处理**：首先，你需要对训练集和测试集进行预处理。这可能涉及到清理、缩放、标准化或归一化数据，以及移除异常...
2025-08-08 07:04:40,318 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 697.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:40,318 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:41,313 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_092] 命中: 894.06ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:41,313 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #93 ===
2025-08-08 07:04:41,313 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_092 | 用户: user_002
2025-08-08 07:04:41,313 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 神经网络如何工作？
2025-08-08 07:04:41,313 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 神经网络是一种模仿人脑神经元行为的计算模型，它通过学习大量的数据来实现对复杂任务的自动处理。以下是一个简化的神经网络工作流程：

1. **输入和输出**：首先...
2025-08-08 07:04:41,314 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 894.1ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:41,314 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:42,224 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_093] 命中: 808.28ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #94 ===
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_093 | 用户: user_002
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 大语言模型（Deep Learning Model）是一种基于深度学习的计算机程序，其主要目标是通过分析和理解文本或语音数据来生成人类级别的自然语言处理（NLP...
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 808.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:42,224 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:43,513 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_094] 命中: 1187.53ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #95 ===
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_094 | 用户: user_003
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 深度学习的基本原理是什么？
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 深度学习是一种机器学习技术，其基本原理是模仿人脑神经网络的结构和功能，通过多层次的神经元连接，使计算机可以从大量数据中自动提取特征，并利用这些特征进行模式识别和...
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1187.5ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 95 | 成功: 95 (100.0%) | 平均耗时: 1251.4ms
2025-08-08 07:04:43,513 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:44,668 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_095] 命中: 1052.72ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #96 ===
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_095 | 用户: user_003
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是大语言模型？
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: “大语言模型”（Deep Learning Model）是一种深度学习技术，它使用大量的文本数据和统计学方法来模拟人类的自然语言处理能力。这种技术的主要目标是理...
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 1052.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:44,668 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:45,433 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_096] 命中: 662.34ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #97 ===
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_096 | 用户: user_003
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 如何优化模型性能？
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 要优化模型性能，可以采取以下几种方法：

1. 数据预处理：数据预处理是提高模型性能的重要步骤。它包括清洗和转换数据，例如删除无效值、填充缺失值、归一化数据等。...
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 662.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:45,433 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:45,433 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 负载阶段: 回归集中访问 | 活跃用户: 5个 | 每用户session: 1 | 请求间隔: 100ms | 进度: 97/100
2025-08-08 07:04:46,288 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_097] 命中: 753.3ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #98 ===
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_097 | 用户: user_003
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它使用统计和数学方法让计算机系统从数据中自动学习模式、规律和知识，从而实现自主决策或预测的能力。简单来说，机器学习是让计算机通过分析...
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 753.3ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:46,288 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:47,363 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_098] 命中: 973.39ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #99 ===
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_098 | 用户: user_003
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是注意力机制？
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 注意力机制（Attention Mechanism）是机器学习和人工智能中的一个核心概念，它是一种算法或模型，用于在给定的输入数据中识别出具有特定重要性的信息。...
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 973.4ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:47,364 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:47,465 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID调整检查: 当前速率=1.48, 历史均值=1.24, 扩容阈值=1.67, 缩容阈值=0.80, 当前缓存=5
2025-08-08 07:04:48,054 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - [req_099] 命中: 587.72ms (策略=FLUID, 缓存大小=5)
2025-08-08 07:04:48,054 INFO  com.infertuner.sink.SimpleResultSink                         [] - === 结果 #100 ===
2025-08-08 07:04:48,054 INFO  com.infertuner.sink.SimpleResultSink                         [] - 请求ID: req_099 | 用户: user_001
2025-08-08 07:04:48,054 INFO  com.infertuner.sink.SimpleResultSink                         [] - 问题: 什么是机器学习？
2025-08-08 07:04:48,054 INFO  com.infertuner.sink.SimpleResultSink                         [] - 回答: 机器学习是一种人工智能技术，它允许计算机从数据中自动学习模式和规律，从而实现对未知数据的预测和分类。它的基本思想是使用算法和统计模型来构建一个系统，该系统可以从...
2025-08-08 07:04:48,055 INFO  com.infertuner.sink.SimpleResultSink                         [] - 状态: 成功 | 耗时: 587.7ms | 模型: Qwen1.5-1.8B-Chat-Hit
2025-08-08 07:04:48,055 INFO  com.infertuner.sink.SimpleResultSink                         [] - --- 统计 ---
2025-08-08 07:04:48,055 INFO  com.infertuner.sink.SimpleResultSink                         [] - 总数: 100 | 成功: 100 (100.0%) | 平均耗时: 1229.2ms
2025-08-08 07:04:48,055 INFO  com.infertuner.sink.SimpleResultSink                         [] - ================
2025-08-08 07:04:48,055 INFO  com.infertuner.source.CacheAwareRequestSource                [] - 优化负载请求流生成完成，共 100 个请求
2025-08-08 07:05:08,543 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - === 最终统计 (策略: FLUID) ===
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - 总请求: 100
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - 缓存命中: 54 (54.0%)
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - 平均延迟: 1229.2ms
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - 最终缓存大小: 5
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - FLUID策略统计: 历史平均速率=1.24请求/秒
2025-08-08 07:05:08,544 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - ================
2025-08-08 07:05:08,547 INFO  com.infertuner.processor.CacheEnabledInferenceProcessor      [] - KV缓存推理服务已关闭
2025-08-08 07:05:08,548 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FINISHED.
2025-08-08 07:05:08,549 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 (0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
2025-08-08 07:05:08,550 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Cache-Aware Request Source -> Cache-Enabled Inference Processor -> Sink: Result Sink (1/1)#0 0bdedd4f6b5af805e5141c34e6c0d210_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
2025-08-08 07:05:08,609 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=364.800mb (382520517 bytes), taskOffHeapMemory=0 bytes, managedMemory=343.040mb (359703515 bytes), networkMemory=85.760mb (89925878 bytes)}, allocationId: 098b54832efa797287b1e67ca48c074e, jobId: 8cdcbaa03c001de0ef964b3836c8f93f).
2025-08-08 07:05:08,611 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 8cdcbaa03c001de0ef964b3836c8f93f from job leader monitoring.
2025-08-08 07:05:08,611 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 8cdcbaa03c001de0ef964b3836c8f93f.
