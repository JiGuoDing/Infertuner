"""
æœ‰æœåŠ¡å¯åŠ¨å¼€é”€çš„æ‰¹é‡æ¨ç†æœåŠ¡
æ¯æ¬¡æ‰¹å¤„ç†è°ƒç”¨éƒ½æœ‰å›ºå®šçš„å¯åŠ¨å¼€é”€ï¼Œæ”’æ‰¹å¯ä»¥åˆ†æ‘Šè¿™ä¸ªå¼€é”€
"""

import os
import sys
import json
import time
import torch
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger(__name__)

class BatchInferenceServiceWithStartupCost:
    """æœ‰æœåŠ¡å¯åŠ¨å¼€é”€çš„æ‰¹é‡æ¨ç†æœåŠ¡"""

    def __init__(self, model_path: str, gpu_id: int = 0):
        self.model_path = model_path
        self.gpu_id = gpu_id

        # GPUè®¾å¤‡é…ç½®
        if torch.cuda.is_available():
            self.device = torch.device(f'cuda:{gpu_id}')
            logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(gpu_id)}")
        else:
            self.device = torch.device('cpu')
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")

        # åŠ è½½çœŸå®æ¨¡å‹
        self.tokenizer, self.model = self._load_model()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_batches": 0,
            "total_requests": 0,
            "total_startup_cost": 0.0,
            "total_inference_time": 0.0,
            "start_time": time.time()
        }

        logger.info("BatchInferenceServiceWithStartupCoståˆå§‹åŒ–å®Œæˆ")

    def _load_model(self):
        """åŠ è½½çœŸå®æ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½æ¨¡å‹: {self.model_path}")

            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            model.eval()

            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
                allocated = torch.cuda.memory_allocated(self.device) / 1e9
                logger.info(f"GPUæ˜¾å­˜: {allocated:.1f}GB / {gpu_memory:.1f}GB")

            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return tokenizer, model

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def batch_generate_response(self, batch_request_data):
        """å¤„ç†æ‰¹é‡è¯·æ±‚ - å…³é”®ï¼šæœ‰å›ºå®šçš„æœåŠ¡å¯åŠ¨å¼€é”€"""
        batch_start_time = time.time()
        self.stats["total_batches"] += 1

        try:
            requests = batch_request_data.get('requests', [])
            batch_size = len(requests)
            batch_id = batch_request_data.get('batch_id', 'unknown')

            if not requests:
                raise ValueError("æ‰¹é‡è¯·æ±‚ä¸èƒ½ä¸ºç©º")

            self.stats["total_requests"] += batch_size

            logger.info(f"ğŸš€ æ”¶åˆ°æ‰¹é‡è¯·æ±‚: batch_id={batch_id}, size={batch_size}")

            # ğŸ”§ å…³é”®ï¼šæ¨¡æ‹Ÿæ¯æ¬¡æ‰¹å¤„ç†è°ƒç”¨çš„å›ºå®šå¯åŠ¨å¼€é”€
            startup_cost_ms = 300  # æ¯æ¬¡è°ƒç”¨å›ºå®š300msçš„å¯åŠ¨å¼€é”€
            logger.info(f"â³ æœåŠ¡å¯åŠ¨å¼€é”€: {startup_cost_ms}ms (æ¯æ¬¡æ‰¹å¤„ç†è°ƒç”¨éƒ½æœ‰è¿™ä¸ªå¼€é”€)")
            time.sleep(startup_cost_ms / 1000.0)
            self.stats["total_startup_cost"] += startup_cost_ms

            # ç„¶åä¸²è¡Œå¤„ç†æ¯ä¸ªè¯·æ±‚
            logger.info(f"ğŸ“ å¯åŠ¨å¼€é”€å®Œæˆï¼Œå¼€å§‹ä¸²è¡Œå¤„ç† {batch_size} ä¸ªè¯·æ±‚...")

            batch_responses = []
            total_inference_time = 0

            for i, req in enumerate(requests):
                logger.info(f"  å¤„ç†ç¬¬ {i+1}/{batch_size} ä¸ªè¯·æ±‚: {req.get('request_id', 'unknown')}")

                # å•ä¸ªè¯·æ±‚çš„æ¨ç†
                single_start = time.time()
                response_text = self._generate_real_text(req)
                single_end = time.time()

                single_inference_time = (single_end - single_start) * 1000
                total_inference_time += single_inference_time

                single_response = {
                    "response": response_text,
                    "inference_time_ms": round(single_inference_time, 2),
                    "request_id": req.get('request_id', f'req_{i}'),
                    "success": True,
                    "timestamp": int(time.time() * 1000)
                }
                batch_responses.append(single_response)

                logger.info(f"  âœ… ç¬¬ {i+1} ä¸ªè¯·æ±‚å®Œæˆ: {single_inference_time:.1f}ms")

            batch_end_time = time.time()
            total_batch_time = (batch_end_time - batch_start_time) * 1000

            result = {
                "responses": batch_responses,
                "batch_size": batch_size,
                "batch_id": batch_id,
                "total_inference_time_ms": round(total_batch_time, 2),
                "success": True,
                "timestamp": int(time.time() * 1000),
                "error": "No error"
            }

            logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: batch_id={batch_id}")
            logger.info(f"ğŸ“Š æ€»è€—æ—¶={total_batch_time:.1f}ms (å¯åŠ¨:{startup_cost_ms}ms + æ¨ç†:{total_inference_time:.1f}ms)")
            logger.info(f"ğŸ’¡ æ”’æ‰¹æ•ˆæœ: {batch_size}ä¸ªè¯·æ±‚å…±äº«{startup_cost_ms}mså¯åŠ¨å¼€é”€ = å¹³å‡{startup_cost_ms/batch_size:.1f}ms/req")

            return result

        except Exception as e:
            logger.error(f"æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
            return {
                "error": str(e),
                "responses": [],
                "batch_size": 0,
                "batch_id": batch_request_data.get('batch_id', 'unknown'),
                "total_inference_time_ms": 0.0,
                "success": False,
                "timestamp": int(time.time() * 1000)
            }

    def _generate_real_text(self, request):
        """çœŸå®æ¨¡å‹æ¨ç†"""
        try:
            user_message = request.get('user_message', '')
            max_tokens = request.get('max_tokens', 100)

            if not user_message:
                raise ValueError("user_messageä¸èƒ½ä¸ºç©º")

            messages = [{"role": "user", "content": user_message}]

            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

            generation_config = {
                "max_new_tokens": max_tokens,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "pad_token_id": self.tokenizer.eos_token_id,
            }

            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)

            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

            return response.strip()

        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        uptime = time.time() - self.stats["start_time"]
        avg_startup_cost = (
            self.stats["total_startup_cost"] / self.stats["total_batches"]
            if self.stats["total_batches"] > 0 else 0.0
        )

        return {
            "total_batches": self.stats["total_batches"],
            "total_requests": self.stats["total_requests"],
            "avg_startup_cost_ms": round(avg_startup_cost, 2),
            "total_startup_cost_ms": round(self.stats["total_startup_cost"], 2),
            "uptime_seconds": round(uptime, 1),
            "gpu_id": self.gpu_id
        }

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 3:
        logger.error("ç”¨æ³•: python3 batch_inference_service.py <node_ip> <model_path> [gpu_id]")
        sys.exit(1)

    node_ip = sys.argv[1]
    model_path = sys.argv[2]
    gpu_id = int(sys.argv[3]) if len(sys.argv) > 3 else 0

    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    # æ£€æŸ¥ç¯å¢ƒ
    if not Path(model_path).exists():
        logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)

    if not torch.cuda.is_available():
        logger.error("CUDAä¸å¯ç”¨ï¼")
        sys.exit(1)

    if gpu_id >= torch.cuda.device_count():
        logger.error(f"GPU {gpu_id} ä¸å­˜åœ¨ï¼")
        sys.exit(1)

    try:
        service = BatchInferenceServiceWithStartupCost(model_path, gpu_id)
        logger.info("ğŸš€ æœ‰å¯åŠ¨å¼€é”€çš„æ¨ç†æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
        logger.info("ğŸ’¡ æ”’æ‰¹ä¼˜åŠ¿: å¤šä¸ªè¯·æ±‚å…±äº«å›ºå®šçš„300msæœåŠ¡å¯åŠ¨å¼€é”€")

        """
            private static class BatchRequestData {
                public List<SingleRequestData> requests;
                public int batch_size;
                public String batch_id;
            }

            private static class SingleRequestData {
                public String user_message;
                public int max_tokens;
                public String request_id;
            }
        """

        for line_num, line in enumerate(sys.stdin, 1):
            line = line.strip()
            if not line:
                continue

            try:
                batch_request_data = json.loads(line)

#                 command = batch_request_data.get("command")
#
#                 if command == "stats":
#                     stats = service.get_stats()
#                     print(json.dumps({"stats": stats}))
#                     sys.stdout.flush()
#                     continue
#
#                 if command == "shutdown":
#                     logger.info("æ”¶åˆ°å…³é—­å‘½ä»¤")
#                     break

                # å¤„ç†æ”’æ‰¹è¯·æ±‚
                result = service.batch_generate_response(batch_request_data)

                print(json.dumps(result))
                sys.stdout.flush()

            except json.JSONDecodeError as e:
                error_result = {
                    "error": f"JSONè§£æé”™è¯¯: {e}",
                    "responses": [],
                    "success": False,
                    "batch_id": "json_error",
                    "timestamp": int(time.time() * 1000)
                }
                print(json.dumps(error_result))
                sys.stdout.flush()

            except Exception as e:
                logger.error(f"å¤„ç†è¯·æ±‚å‡ºé”™: {e}")
                error_result = {
                    "error": f"å¤„ç†é”™è¯¯: {e}",
                    "responses": [],
                    "success": False,
                    "batch_id": "processing_error",
                    "timestamp": int(time.time() * 1000)
                }
                print(json.dumps(error_result))
                sys.stdout.flush()

    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

    logger.info("ğŸ›‘ æ¨ç†æœåŠ¡åœæ­¢")

if __name__ == "__main__":
    main()