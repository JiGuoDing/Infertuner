"""
DS2 vs InferTuner
DS2: åŸºäºçœŸå®å¤„ç†ç‡çš„å¹¶è¡Œåº¦ä¼˜åŒ– (OSDI'18)
InferTuner: è”åˆä¼˜åŒ–å¹¶è¡Œåº¦å’Œæ‰¹å¤§å°çš„åŠ¨æ€è§„åˆ’æ–¹æ³•ï¼ˆä½¿ç”¨çœŸå®æ€§èƒ½æ•°æ®ä»£æ›¿ï¼‰

ä½¿ç”¨æ–¹æ³•:
python3 infertuner_validator.py ../data/performance_profiling/performance_matrix_20250817_131935.csv

"""

import os
import sys
import math

import numpy as np
import pandas as pd
from loguru import logger
from collections import deque
from dataclasses import dataclass
from typing import Tuple, Optional, List
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error


@dataclass
class Config:
    """é…ç½®ç»“æœ"""
    p: int  # å¹¶è¡Œåº¦ï¼ˆGPUæ•°é‡ï¼‰
    b: int  # æ‰¹å¤§å°
    cost: float  # æˆæœ¬ï¼ˆGPUæ•°é‡ï¼‰
    predicted_latency: float
    predicted_throughput: float


class PerformanceModel:
    """æ€§èƒ½é¢„æµ‹æ¨¡å‹"""

    def __init__(self, performance_data: pd.DataFrame):
        self.df = performance_data
        self.latency_model = None
        self.throughput_model = None
        self._build_models()

    def _build_models(self):
        """æ„å»ºæœºå™¨å­¦ä¹ æ€§èƒ½é¢„æµ‹æ¨¡å‹"""
        print("ğŸ§  æ„å»ºæ€§èƒ½é¢„æµ‹æ¨¡å‹...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.df[['parallelism', 'batch_size', 'target_rate']].values
        y_latency = self.df['avg_latency_ms'].values
        y_throughput = self.df['throughput_rps'].values

        # è®­ç»ƒå»¶è¿Ÿé¢„æµ‹æ¨¡å‹
        self.latency_model = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.01, max_depth=6, random_state=42
        )
        self.latency_model.fit(X, y_latency)

        # è®­ç»ƒååé‡é¢„æµ‹æ¨¡å‹
        self.throughput_model = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.01, max_depth=6, random_state=42
        )
        self.throughput_model.fit(X, y_throughput)

        # è¯„ä¼°æ¨¡å‹ç²¾åº¦
        X_train, X_test, y_lat_train, y_lat_test, y_thr_train, y_thr_test = train_test_split(
            X, y_latency, y_throughput, test_size=0.2, random_state=42
        )

        lat_pred = self.latency_model.predict(X_test)
        thr_pred = self.throughput_model.predict(X_test)

        lat_mae = mean_absolute_error(y_lat_test, lat_pred)
        lat_mape = mean_absolute_percentage_error(y_lat_test, lat_pred) * 100
        thr_mae = mean_absolute_error(y_thr_test, thr_pred)
        thr_mape = mean_absolute_percentage_error(y_thr_test, thr_pred) * 100

        print(f"   å»¶è¿Ÿæ¨¡å‹: MAE={lat_mae:.1f}ms, MAPE={lat_mape:.1f}%")
        print(f"   ååé‡æ¨¡å‹: MAE={thr_mae:.2f}req/s, MAPE={thr_mape:.1f}%")

    def predict(self, p: int, b: int, target_rate: float) -> Tuple[float, float]:
        """é¢„æµ‹ç»™å®šé…ç½®çš„æ€§èƒ½"""
        X = np.array([[p, b, target_rate]])
        latency = self.latency_model.predict(X)[0]
        throughput = self.throughput_model.predict(X)[0]
        return latency, throughput


def load_mapping(mapping_file="../data/submit_job_Falcon3-7B-Instruct_1000ms/parallelism_mapping.csv"):
    """
    è·å–æ˜ å°„è¡¨ï¼š å¹¶è¡Œåº¦ -> (ååé‡, å»¶è¿Ÿ)
    :param mapping_file:
    :return:
    """
    df = pd.read_csv(mapping_file)
    mapping = {row['parallelism']: (row['throughput_rps'], row['avg_latency_ms']) for _, row in df.iterrows()}
    return mapping


# æ ¹æ® parallelism æŸ¥è¯¢ throughput å’Œ latency
def get_perf_by_parallelism(parallelism, mapping):
    """åœ¨ContTuneä¸­ä½¿ç”¨çš„å®é™…æ€§èƒ½æµ‹é‡å‡½æ•°"""
    if parallelism in mapping:
        return mapping[parallelism]
    else:
        raise ValueError(f"Parallelism {parallelism} not found in mapping table")


class DS2Algorithm:
    """
    DS2ç®—æ³•å®ç° (OSDI'18)
    æ ¸å¿ƒæ€æƒ³ï¼šåŸºäºçœŸå®å¤„ç†ç‡ä¼˜åŒ–å¹¶è¡Œåº¦ï¼Œæ‰¹å¤§å°å›ºå®šä¸º1
    """

    def __init__(self, performance_data: pd.DataFrame, max_parallelism: int):
        self.df_b1 = performance_data[performance_data["batch_size"] == 1].copy()
        self.performance_model = PerformanceModel(performance_data)
        self.max_parallelism = max_parallelism
        self._build_true_rate_model()

    def _build_true_rate_model(self):
        """æ„å»ºDS2çš„çœŸå®å¤„ç†ç‡æ¨¡å‹"""
        print("ğŸ”„ æ„å»ºDS2çœŸå®å¤„ç†ç‡æ¨¡å‹...")

        # DS2åªä½¿ç”¨batch_size=1çš„æ•°æ®
        # df_b1 = self.df[self.df['batch_size'] == 1].copy()

        if len(self.df_b1) == 0:
            raise ValueError("DS2éœ€è¦batch_size=1çš„æ€§èƒ½æ•°æ®")

        # è®¡ç®—æ¯ä¸ªå¹¶è¡Œåº¦çš„å•å®ä¾‹çœŸå®å¤„ç†ç‡
        self.true_rate_per_instance = {}
        for p in self.df_b1['parallelism'].unique():
            p_data = self.df_b1[self.df_b1['parallelism'] == p]
            # DS2å‡è®¾ï¼šåœ¨æ— backpressureæ—¶ï¼Œå®é™…ååé‡å°±æ˜¯çœŸå®å¤„ç†ç‡
            avg_throughput = p_data['throughput_rps'].mean()
            # å•å®ä¾‹çš„å®é™…ååé‡ï¼ˆçœŸå®å¤„ç†ç‡ï¼‰
            single_instance_rate = avg_throughput / p
            self.true_rate_per_instance[p] = single_instance_rate
            print(f"p={p}: å•å®ä¾‹çœŸå®å¤„ç†ç‡={single_instance_rate:.3f} req/s")

    def estimate_true_processing_rate(self, parallelism: int) -> float:
        """
        DS2æ ¸å¿ƒï¼šä¼°ç®—çœŸå®å¤„ç†ç‡
        å‡è®¾çº¿æ€§æ‰©å±•ï¼štotal_rate = single_instance_rate * parallelism
        """

        # å¦‚æœæ˜¯å½“å‰å·²çŸ¥çš„å¹¶è¡Œåº¦ï¼Œç›´æ¥è¿”å›çº¿æ€§æ‰©å±•çš„çœŸæ˜¯å¤„ç†ç‡
        if parallelism in self.true_rate_per_instance:
            return self.true_rate_per_instance[parallelism] * parallelism

        # å¦åˆ™ï¼Œè¿›è¡Œçº¿æ€§æ’å€¼ä¼°ç®—
        # è·å–å·²çŸ¥çš„å¹¶è¡Œåº¦åˆ—è¡¨
        known_p = list(self.true_rate_per_instance.keys())
        # å¦‚æœå½“å‰è¿˜æ²¡æœ‰æ€§èƒ½æ•°æ®
        if not known_p:
            return 1.0

        # å½“å‰å¹¶è¡Œåº¦ä¸åœ¨å·²çŸ¥èŒƒå›´å†…
        # æ–¹æ¡ˆ1.çº¿æ€§æ’å€¼
        #         known_rates = [self.true_rate_per_instance[p] for p in known_p]
        #         estimated_single_rate = np.interp(parallelism, known_p, known_rates)
        #         return estimated_single_rate * parallelism

        # æ–¹æ¡ˆ2.ä½¿ç”¨æœ€æ¥è¿‘çš„é…ç½®
        closest_p = min(known_p, key=lambda x: abs(x - parallelism))
        single_rate = self.true_rate_per_instance[closest_p]
        return single_rate * parallelism

    def ds2_scaling_decision(self, target_rate: float, target_slo: float) -> Optional[Config]:
        """
        DS2æ ¸å¿ƒç®—æ³•ï¼šåŸºäºçœŸå®å¤„ç†ç‡è®¡ç®—æœ€ä¼˜å¹¶è¡Œåº¦

        DS2å…¬å¼ï¼šoptimal_parallelism = target_rate / true_rate_per_instance
        çº¦æŸï¼šfixed batch_size = 1
        """
        print(f"\nğŸ”„ DS2ç®—æ³•")
        print(f"   ç›®æ ‡: {target_rate} req/s, SLO â‰¤ {target_slo}ms")
        print(f"   çº¦æŸ: æ‰¹å¤§å°å›ºå®š b=1")

        feasible_configs = []
        batch_size = 1  # DS2æ ¸å¿ƒçº¦æŸ

        # DS2æœç´¢ç©ºé—´ï¼šåªèƒ½è°ƒæ•´å¹¶è¡Œåº¦
        available_p = sorted(self.true_rate_per_instance.keys())
        max_p = max(available_p) if available_p else self.max_parallelism

        for p in range(1, max_p + 1):
            # DS2æ ¸å¿ƒï¼šåŸºäºçœŸå®å¤„ç†ç‡åˆ¤æ–­
            true_processing_rate = self.estimate_true_processing_rate(p)

            # DS2çº¦æŸæ£€æŸ¥
            throughput_ok = true_processing_rate >= target_rate * 0.95  # 5%å®¹å·®

            if not throughput_ok:
                print(
                    f"   âŒ p={p} ä¸æ»¡è¶³ååé‡çº¦æŸ: çœŸå®å¤„ç†ç‡={true_processing_rate:.2f}req/s < {target_rate * 0.95:.2f}req/s")
                continue

            # ä½¿ç”¨æ€§èƒ½æ¨¡å‹ä¼°ç®—å»¶è¿Ÿ
            pred_latency, _ = self.performance_model.predict(p, batch_size, target_rate)
            latency_ok = pred_latency <= target_slo

            if not latency_ok:
                print(f"   âŒ p={p} ä¸æ»¡è¶³å»¶è¿Ÿçº¦æŸ: é¢„æµ‹å»¶è¿Ÿ={pred_latency:.0f}ms > {target_slo:.0f}ms")
                continue

            cost = p  # GPUæ•°é‡
            config = Config(p, batch_size, cost, pred_latency, true_processing_rate)
            feasible_configs.append(config)
            print(f"   âœ… å¯è¡Œ: p={p}, b={batch_size}, æˆæœ¬={cost}GPU, "
                  f"çœŸå®å¤„ç†ç‡={true_processing_rate:.2f}req/s, å»¶è¿Ÿâ‰ˆ{pred_latency:.0f}ms")

        if not feasible_configs:
            print(f"   âŒ DS2æ— å¯è¡Œé…ç½®")
            return None

        # DS2é€‰æ‹©æœ€å°å¹¶è¡Œåº¦ï¼ˆæœ€å°æˆæœ¬ï¼‰
        best_config = min(feasible_configs, key=lambda x: x.cost)
        print(f"   ğŸ† DS2æœ€ä¼˜: p={best_config.p}, b={best_config.b}, æˆæœ¬={best_config.cost}GPU")

        return best_config


class GaussianProcessModel:
    """
    é«˜æ–¯è¿‡ç¨‹å›å½’æ¨¡å‹ï¼Œç”¨äº ContTune ç®—æ³•ä¸­é…ç½®ä¸æ€§èƒ½çš„æ˜ å°„; åŒæ—¶é¢„æµ‹ååé‡å’Œå»¶è¿Ÿ
    """

    def __init__(self, performance_data, parallelism_search_space: np.ndarray):
        """
        åˆå§‹åŒ–å¹¶è‡ªåŠ¨è®­ç»ƒ GP æ¨¡å‹
        :param performance_data: æ€§èƒ½é‡‡é›†æ•°æ®
        """
        self.df = performance_data
        self.parallelism_search_space = parallelism_search_space
        self.scaler_X = StandardScaler()
        self.update(new_data=performance_data)

    def update(self, new_data: pd.DataFrame, prewarm: bool = False):
        """
        å¢é‡æ›´æ–°æ•°æ®å¹¶é‡æ–°è®­ç»ƒ GP æ¨¡å‹
        :param new_data: æ–°å¢æ•°æ® DataFrame
        :param prewarm: æ˜¯å¦æ·»åŠ é¢„çƒ­æ ·æœ¬ï¼ˆmin/max parallelismï¼‰
        """
        # æ•°æ®æ¸…æ´—
        new_data = new_data.dropna()

        # æ·»åŠ é¢„çƒ­æ ·æœ¬
        if prewarm and len(new_data) > 0:
            min_p = new_data["parallelism"].min()
            max_p = new_data["parallelism"].max()
            prewarm_points = []
            for p in [min_p, max_p]:
                row = new_data[new_data["parallelism"] == p]
                if not row.empty:
                    prewarm_points.append(row.iloc[0])
            if prewarm_points:
                prewarm_df = pd.DataFrame(prewarm_points)
                new_data = pd.concat([new_data, prewarm_df], ignore_index=True)

        # åˆå§‹åŒ–æˆ–å¢é‡æ›´æ–°è®­ç»ƒæ•°æ®
        X_new = new_data[["parallelism"]].values
        y_throughput_new = new_data[["throughput_rps"]].values
        y_latency_new = new_data[["avg_latency_ms"]].values

        if not hasattr(self, "X_train"):
            self.X_train = X_new
            self.y_train_throughput = y_throughput_new
            self.y_train_latency = y_latency_new
        else:
            self.X_train = np.vstack((self.X_train, X_new))
            self.y_train_throughput = np.vstack((self.y_train_throughput, y_throughput_new))
            self.y_train_latency = np.vstack((self.y_train_latency, y_latency_new))

        # æ ‡å‡†åŒ– X
        self.X_train_scaled = self.scaler_X.fit_transform(self.X_train)

        # å®šä¹‰æ ¸å‡½æ•°
        kernel = C(1.0, (1e-2, 1e2)) * RBF(1.0, (1e-4, 1e4))

        # GP æ¨¡å‹: ååé‡
        self.gp_throughput = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=25,
            alpha=1e-2,
            normalize_y=True
        )
        self.gp_throughput.fit(self.X_train_scaled, self.y_train_throughput)

        # GP æ¨¡å‹: å»¶è¿Ÿ
        self.gp_latency = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=25,
            alpha=1e-2,
            normalize_y=True
        )
        self.gp_latency.fit(self.X_train_scaled, self.y_train_latency)

    def predict(self, x):
        """
        é¢„æµ‹ååé‡å’Œå»¶è¿Ÿ
        :param x:
        :return:
        """
        X = np.array(x).reshape(-1, 1)
        throughput_mean, throughput_std = self.gp_throughput.predict(X, return_std=True)
        latency_mean, latency_std = self.gp_latency.predict(X, return_std=True)

        return throughput_mean, throughput_std, latency_mean, latency_std

    def suggest_next_parallelism(self, kappa=1.96):
        """
        ä½¿ç”¨ Upper Confidence Bound (UCB) ç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªå¹¶è¡Œåº¦ï¼Œç»¼åˆè€ƒè™‘ååé‡å’Œå»¶è¿Ÿ
        :param kappa: UCB æ¢ç´¢-åˆ©ç”¨æƒè¡¡å‚æ•°
        :return: æ¨èçš„å¹¶è¡Œåº¦
        """
        # é¢„æµ‹æœç´¢ç©ºé—´ä¸­çš„ååé‡å’Œå»¶è¿Ÿ
        X = np.array(self.parallelism_search_space).reshape(-1, 1)
        throughput_mean, throughput_std = self.gp_throughput.predict(X, return_std=True)
        latency_mean, latency_std = self.gp_latency.predict(X, return_std=True)

        # æ ‡å‡†åŒ–ååé‡å’Œå»¶è¿Ÿä»¥ä¾¿æ¯”è¾ƒï¼ˆå› ä¸ºååé‡å’Œå»¶è¿Ÿçš„é‡çº²ä¸åŒï¼‰
        throughput_mean_norm = (throughput_mean - throughput_mean.mean()) / throughput_mean.std()
        throughput_std_norm = throughput_std / throughput_mean.std()
        latency_mean_norm = (latency_mean - latency_mean.mean()) / latency_mean.std()
        latency_std_norm = latency_std / latency_mean.std()

        # è®¡ç®— UCB åˆ†æ•°ï¼Œååé‡æœ€å¤§åŒ–ï¼ˆæ­£å‘ï¼‰ï¼Œå»¶è¿Ÿæœ€å°åŒ–ï¼ˆè´Ÿå‘ï¼‰
        throughput_ucb = throughput_mean_norm + kappa * throughput_std_norm
        latency_ucb = -latency_mean_norm + kappa * latency_std_norm  # è´Ÿå·è¡¨ç¤ºå»¶è¿Ÿè¶Šå°è¶Šå¥½
        combined_ucb = throughput_ucb + latency_ucb

        # é€‰æ‹© UCB åˆ†æ•°æœ€é«˜çš„å¹¶è¡Œåº¦
        best_index = np.argmax(combined_ucb)
        return self.parallelism_search_space[best_index]


class ContTuneAlgorithm:
    """
    ContTuneç®—æ³•å®ç°
    æ ¸å¿ƒæ€æƒ³ï¼šBig-Small ç®—æ³• + CBO
    åªæ¶‰åŠå¹¶è¡Œåº¦è°ƒæ•´ï¼Œæ‰¹å¤§å°å›ºå®šä¸º1
    """

    # TODO å®ç° ContTune ç®—æ³•
    def __init__(self,
                 measure_fn,
                 target_throughput,
                 slo,
                 performance_data,
                 max_parallelism: int = 19,
                 min_parallelism: int = 1,
                 big_multiplier: int = 2,
                 small_max_iters: int = 3,
                 history_max_len: int = 10):
        """
        åˆå§‹åŒ– ContTune ç®—æ³•
        :param target_throughput: ç›®æ ‡ååé‡
        :param measure_fn: æ€§èƒ½æµ‹é‡å‡½æ•°ï¼Œè¾“å…¥å¹¶è¡Œåº¦ï¼Œè¾“å‡ºçœŸå®ååé‡å’Œå»¶è¿Ÿ
        :param performance_data: æ€§èƒ½é‡‡é›†æ•°æ®
        :param max_parallelism: æœ€å¤§å¹¶è¡Œåº¦
        :param min_parallelism: æœ€å°å¹¶è¡Œåº¦
        :param big_multiplier: Big Phase çš„å¹¶è¡Œåº¦æ”¾å¤§ç³»æ•°
        :param small_max_iters: Small Phase çš„æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.df_b1 = performance_data[performance_data["batch_size"] == 1].copy()
        self.target_throughput = target_throughput
        self.slo = slo
        self.measure_fn = measure_fn
        # è®¾ç½®è¿è¡Œæ—¶å‚æ•°
        self.parallelism_search_space = np.arange(min_parallelism, max_parallelism + 1)
        self.min_parallelism = min_parallelism
        self.max_parallelism = max_parallelism
        self.big_multiplier = big_multiplier
        self.small_max_iters = small_max_iters

        self.performance_model = PerformanceModel(performance_data=performance_data)

        # åˆå§‹åŒ–å†å²æ•°æ®ï¼Œåªç»´æŠ¤ history_max_len æ¡è®°å½•
        self.history = deque(maxlen=history_max_len)

        # åˆ›å»º GP æ¨¡å‹
        self.gp = GaussianProcessModel(
            performance_data=self.df_b1,
            parallelism_search_space=self.parallelism_search_space
        )

        # åŠ è½½çœŸå®æ€§èƒ½æ˜ å°„è¡¨
        self.mapping = load_mapping()

    def big_phase(self, start_parallelism):
        """
        Big Phase: æ”¾å¤§å¹¶è¡Œåº¦ï¼Œç›´åˆ°ååé‡å’Œå»¶è¿ŸåŒæ—¶æ»¡è¶³ SLA
        :param start_parallelism: èµ·å§‹å¹¶è¡Œåº¦
        :return: (æœ€ç»ˆå¹¶è¡Œåº¦, ååé‡, å»¶è¿Ÿ)
        """
        current_parallelism = start_parallelism
        current_throughput, current_latency = self.measure_fn(current_parallelism)
        self.history.append((current_parallelism, current_throughput, current_latency))

        logger.info(
            f"[BIG] start with p={current_parallelism}, throughput={current_throughput:.2f} req/s, latency={current_latency:.2f} ms")

        iter_count = 0
        while current_throughput < self.target_throughput or current_latency > self.slo:
            iter_count += 1
            max_history_parallelism = max(p for p, _, _ in self.history) if self.history else current_parallelism

            # æ”¾å¤§å¹¶è¡Œåº¦
            if current_parallelism >= max_history_parallelism:
                current_parallelism = min(
                    max(math.ceil(max_history_parallelism * self.big_multiplier), max_history_parallelism + 1),
                    self.max_parallelism)
            else:
                current_parallelism = max_history_parallelism

            current_throughput, current_latency = self.measure_fn(current_parallelism)
            self.history.append((current_parallelism, current_throughput, current_latency))

            # è®°å½• SLA çŠ¶æ€
            if current_latency > self.slo:
                reason = f"latency {current_latency:.2f} > threshold {self.slo}"
                logger.warning(f"[BIG] SLA warning at p={current_parallelism} ({reason})")
            if current_throughput < self.target_throughput:
                reason = f"throughput {current_throughput:.2f} < target {self.target_throughput}"
                logger.warning(f"[BIG] SLA warning at p={current_parallelism} ({reason})")

            logger.info(
                f"[BIG] iter {iter_count}: p={current_parallelism}, throughput={current_throughput:.2f} req/s, latency={current_latency:.2f} ms")

            if current_parallelism >= self.max_parallelism:
                logger.warning("[BIG] reached maximum parallelism, stopping Big Phase")
                break

        # æœ€ç»ˆæ£€æŸ¥ SLA
        if current_throughput < self.target_throughput or current_latency > self.slo:
            reason = []
            if current_throughput < self.target_throughput:
                reason.append(f"throughput {current_throughput:.2f} < target {self.target_throughput}")
            if current_latency > self.slo:
                reason.append(f"latency {current_latency:.2f} > threshold {self.slo}")
            logger.warning(f"[BIG] final Big Phase SLA check: {', '.join(reason)}")

        return current_parallelism, current_throughput, current_latency

    def small_phase(self, start_parallelism):
        """
        Small Phase: åœ¨ Big Phase çš„ç»“æœåŸºç¡€ä¸Šï¼Œå°è¯•å‡å°‘å¹¶è¡Œåº¦ä»¥æ‰¾åˆ°æœ€å°çš„ SLA æ»¡è¶³ç‚¹ã€‚

        SLA: throughput >= target_throughput AND latency <= latency_threshold

        :param start_parallelism: Big Phase ç»“æŸæ—¶çš„å¹¶è¡Œåº¦
        :return: (æœ€ä½³å¹¶è¡Œåº¦, ååé‡, å»¶è¿Ÿ)
        """

        current_parallelism = start_parallelism
        tested_points = {current_parallelism}

        # è®°å½• Big Phase èµ·ç‚¹
        throughput, latency = self.measure_fn(current_parallelism)
        self.history.append((current_parallelism, throughput, latency))
        logger.info(f"[SMALL] start from p={current_parallelism}: throughput={throughput:.2f}, latency={latency:.2f}")

        # å¦‚æœ Big Phase çš„èµ·ç‚¹æœ¬èº«ä¸æ»¡è¶³ SLAï¼Œç›´æ¥è¿”å›
        if not self._meet_sla(throughput, latency):
            reason = self._sla_violation_reason(throughput, latency)
            logger.warning(f"[SMALL] starting point does NOT meet SLA ({reason}), cannot reduce parallelism further.")
            return current_parallelism, throughput, latency

        # å¦‚æœæ»¡è¶³ SLAï¼Œåˆ™å°è¯•å‡å°‘å¹¶è¡Œåº¦
        for it in range(self.small_max_iters):
            logger.info(f"[SMALL] iteration {it + 1}")
            next_parallelism = self.gp.suggest_next_parallelism()

            # å¿…é¡»ä¿è¯å€™é€‰å¹¶è¡Œåº¦ < å½“å‰å¹¶è¡Œåº¦ï¼ˆå¾€ä¸‹è°ƒï¼‰
            if next_parallelism >= current_parallelism or next_parallelism in tested_points:
                logger.info("No smaller parallelism suggested, stopping Small Phase.")
                break

            # å®æµ‹æ€§èƒ½
            throughput, latency = self.measure_fn(next_parallelism)
            logger.info(f"[SMALL] test p={next_parallelism}: throughput={throughput:.2f}, latency={latency:.2f}")

            # æ›´æ–° GP å’Œå†å²
            new_data = pd.DataFrame([[next_parallelism, throughput, latency]],
                                    columns=["parallelism", "throughput_rps", "avg_latency_ms"])
            self.gp.update(new_data)
            self.history.append((next_parallelism, throughput, latency))
            tested_points.add(next_parallelism)

            # å¦‚æœ SLA ä»æ»¡è¶³ï¼Œåˆ™æ›´æ–° current_parallelismï¼ˆç»§ç»­å¾€ä¸‹è°ƒï¼‰
            if self._meet_sla(throughput, latency):
                logger.info(f"[SMALL] SLA still satisfied at p={next_parallelism}. Continue reducing.")
                current_parallelism = next_parallelism
            else:
                reason = self._sla_violation_reason(throughput, latency)
                logger.info(f"[SMALL] SLA violated at p={next_parallelism} ({reason}). Stop.")
                break

        # è¿”å›æœ€å°å¹¶è¡Œåº¦ï¼ˆSLA æ»¡è¶³ï¼‰
        best_point = self._select_min_parallelism_sla()
        logger.info(f"[SMALL] final minimal SLA point: p={best_point[0]}, "
                    f"throughput={best_point[1]:.2f}, latency={best_point[2]:.2f}")
        return best_point

    def _meet_sla(self, throughput, latency):
        return throughput >= self.target_throughput and latency <= self.slo

    def _sla_violation_reason(self, throughput, latency):
        reasons = []
        if throughput < self.target_throughput:
            reasons.append(f"throughput {throughput:.2f} < target {self.target_throughput}")
        if latency > self.slo:
            reasons.append(f"latency {latency:.2f} > threshold {self.slo}")
        return " and ".join(reasons)

    def _select_min_parallelism_sla(self):
        # ç­›é€‰æ»¡è¶³ SLA çš„ç‚¹
        sla_points = [p for p in self.history if self._meet_sla(p[1], p[2])]
        if sla_points:
            # æŒ‰å¹¶è¡Œåº¦å‡åºé€‰æ‹©æœ€å°çš„
            return min(sla_points, key=lambda x: x[0])
        else:
            # å¦‚æœæ²¡æœ‰ä»»ä½•ç‚¹æ»¡è¶³ SLAï¼Œé€€è€Œæ±‚å…¶æ¬¡é€‰æ‹©ååé‡æœ€é«˜çš„
            return max(self.history, key=lambda x: x[1])

    def conttune_scaling_decision(self, start_parallelism: int):
        """
        æ‰§è¡Œ ContTune è°ƒèŠ‚é€»è¾‘ï¼šå…ˆ Big Phase æ”¾å¤§å¹¶è¡Œåº¦ï¼Œå† Small Phase ç²¾è°ƒåˆ°æœ€å°æ»¡è¶³ SLA çš„å¹¶è¡Œåº¦
        :param start_parallelism: åˆå§‹å¹¶è¡Œåº¦
        :return: å¹¶è¡Œåº¦ã€ååé‡ã€å»¶è¿Ÿ
        """
        logger.info("[ContTune] === Start Scaling Decision ===")

        # 1. Big Phase
        big_p, big_thr, big_lat = self.big_phase(start_parallelism=start_parallelism)
        logger.info(f"[ContTune] Big Phase result: p={big_p}, throughput={big_thr:.2f}, latency={big_lat:.2f}")

        # æ£€æŸ¥ Big Phase SLA
        if not self._meet_sla(big_thr, big_lat):
            reason = self._sla_violation_reason(big_thr, big_lat)
            logger.warning(f"[ContTune] Big Phase SLA æœªè¾¾è¦æ±‚: {reason}")
            # å³ä½¿æœªè¾¾ SLAï¼Œä¹Ÿå¯ä»¥å°è¯•è¿”å›å½“å‰æœ€å¤§å¹¶è¡Œåº¦ä½œä¸ºé…ç½®

            final_config = Config(p=big_p, b=1, cost=big_p, predicted_latency=big_lat, predicted_throughput=big_thr)
            return final_config

        # 2. Small Phase
        best_p, best_thr, best_lat = self.small_phase(start_parallelism=big_p)
        logger.info(f"[ContTune] Small Phase result: p={best_p}, throughput={best_thr:.2f}, latency={best_lat:.2f}")

        # æ£€æŸ¥æœ€ç»ˆ SLA
        sla_met = self._meet_sla(best_thr, best_lat)
        reason = None if sla_met else self._sla_violation_reason(best_thr, best_lat)

        final_config = Config(p=best_p, b=1, cost=best_p, predicted_throughput=best_thr, predicted_latency=best_lat)
        if reason:
            logger.warning(f"[ContTune] Big Phase SLA æœªè¾¾è¦æ±‚: {reason}")

        logger.info(f"[ContTune] Final scaling decision: {final_config}")
        return final_config


class InferTunerAlgorithm:
    """
    InferTunerç®—æ³•å®ç°
    æ ¸å¿ƒæ€æƒ³ï¼šè”åˆä¼˜åŒ–å¹¶è¡Œåº¦å’Œæ‰¹å¤§å°ï¼Œä½¿ç”¨åŠ¨æ€è§„åˆ’æ±‚è§£
    """

    def __init__(self, performance_data: pd.DataFrame):
        self.df = performance_data
        self.performance_model = PerformanceModel(performance_data)
        # æŒ‰é…ç½®åˆ†ç»„å–å¹³å‡å€¼
        self.df_avg = self.df.groupby(['parallelism', 'batch_size', 'target_rate']).agg({
            'throughput_rps': 'mean',
            'avg_latency_ms': 'mean'
        }).reset_index()

    def generate_feasible_configs(self, target_rate: float, target_slo: float) -> List[Config]:
        """ç”Ÿæˆæ‰€æœ‰å¯è¡Œçš„(p,b)é…ç½®"""
        feasible_configs = []

        # InferTuneræœç´¢ç©ºé—´ï¼šè”åˆä¼˜åŒ–(p,b)
        p_values = sorted(self.df_avg['parallelism'].unique())
        b_values = sorted(self.df_avg['batch_size'].unique())

        for p in p_values:
            for b in b_values:
                # ä½¿ç”¨æ€§èƒ½æ¨¡å‹é¢„æµ‹
                pred_latency, pred_throughput = self.performance_model.predict(p, b, target_rate)

                # çº¦æŸæ£€æŸ¥
                throughput_ok = pred_throughput >= target_rate * 0.95  # 5%å®¹å·®
                if not throughput_ok:
                    print(
                        f"   âŒ p={p}, b={b} ä¸æ»¡è¶³ååé‡çº¦æŸ: é¢„æµ‹å¤„ç†ç‡={pred_throughput:.2f}req/s < {target_rate * 0.95:.2f}req/s")
                    continue

                latency_ok = pred_latency <= target_slo
                if not latency_ok:
                    print(f"   âŒ p={p}, b={b} ä¸æ»¡è¶³å»¶è¿Ÿçº¦æŸ: é¢„æµ‹å»¶è¿Ÿ={pred_latency:.0f}ms > {target_slo:.0f}ms")
                    continue

                cost = p  # GPUæ•°é‡ä½œä¸ºæˆæœ¬
                config = Config(p, b, cost, pred_latency, pred_throughput)
                feasible_configs.append(config)

        return feasible_configs

    def infertuner_scaling_decision(self, target_rate: float, target_slo: float) -> Optional[Config]:
        """
        InferTuneræ ¸å¿ƒç®—æ³•ï¼šè”åˆä¼˜åŒ–(p,b)
        åŠ¨æ€è§„åˆ’ï¼šåœ¨å¯è¡Œé…ç½®ä¸­é€‰æ‹©æœ€å°æˆæœ¬
        """
        print(f"\nğŸ¯ InferTunerç®—æ³•")
        print(f"   ç›®æ ‡: {target_rate} req/s, SLO â‰¤ {target_slo}ms")
        print(f"   ä¼˜åŠ¿: è”åˆä¼˜åŒ–å¹¶è¡Œåº¦(p)å’Œæ‰¹å¤§å°(b)")

        # ç”Ÿæˆå¯è¡Œé…ç½®
        feasible_configs = self.generate_feasible_configs(target_rate, target_slo)

        if not feasible_configs:
            print(f"   âŒ InferTuneræ— å¯è¡Œé…ç½®")
            return None

        # æ˜¾ç¤ºå¯è¡Œé…ç½®
        print(
            f"   æœç´¢ç©ºé—´: pâˆˆ{sorted(self.df_avg['parallelism'].unique())}, bâˆˆ{sorted(self.df_avg['batch_size'].unique())}")
        for config in feasible_configs:
            print(f"   âœ… å¯è¡Œ: p={config.p}, b={config.b}, æˆæœ¬={config.cost}GPU, "
                  f"ååé‡â‰ˆ{config.predicted_throughput:.2f}req/s, å»¶è¿Ÿâ‰ˆ{config.predicted_latency:.0f}ms")

        # InferTuneré€‰æ‹©æœ€å°æˆæœ¬é…ç½®
        best_config = min(feasible_configs, key=lambda x: x.cost)
        print(f"   ğŸ† InferTuneræœ€ä¼˜: p={best_config.p}, b={best_config.b}, æˆæœ¬={best_config.cost}GPU")

        return best_config


class AlgorithmComparator:
    """ç®—æ³•å¯¹æ¯”å™¨"""

    def __init__(self, performance_data_file: str, max_parallelism: int = 19):
        # åŠ è½½æ•°æ®
        self.df = pd.read_csv(performance_data_file)
        print(f"ğŸ“Š åŠ è½½æ€§èƒ½æ•°æ®: {len(self.df)} æ¡è®°å½•")

        # æ•°æ®æ¸…æ´—
        self.df = self.df[
            (self.df['throughput_rps'] > 0) &
            (self.df['avg_latency_ms'] > 0) &
            (self.df['success_rate_pct'] > 90)
            ].copy()
        print(f"   æ¸…æ´—å: {len(self.df)} æ¡æœ‰æ•ˆè®°å½•")

        # åˆå§‹åŒ–ç®—æ³•
        self.ds2 = DS2Algorithm(self.df, max_parallelism=max_parallelism)
        self.mapping = load_mapping()
        self.measure_fn = lambda p: get_perf_by_parallelism(p, self.mapping)
        self.infertuner = InferTunerAlgorithm(self.df)

        # æ˜¾ç¤ºæ•°æ®èŒƒå›´
        self._show_data_summary()

    def _show_data_summary(self):
        """æ˜¾ç¤ºæ•°æ®æ¦‚å†µ"""
        print(f"\nğŸ“ˆ æ•°æ®æ¦‚å†µ:")
        print(f"   å¹¶è¡Œåº¦èŒƒå›´: {self.df['parallelism'].min()}-{self.df['parallelism'].max()}")
        print(f"   æ‰¹å¤§å°èŒƒå›´: {self.df['batch_size'].min()}-{self.df['batch_size'].max()}")
        print(f"   è¯·æ±‚é€Ÿç‡èŒƒå›´: {self.df['target_rate'].min():.1f}-{self.df['target_rate'].max():.1f} req/s")
        print(f"   å»¶è¿ŸèŒƒå›´: {self.df['avg_latency_ms'].min():.0f}-{self.df['avg_latency_ms'].max():.0f} ms")

    def generate_realistic_scenarios(self) -> List[Tuple[str, float, float]]:
        """åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæµ‹è¯•åœºæ™¯"""
        min_latency = self.df['avg_latency_ms'].min()

        scenarios = [
            ("æä½è´Ÿè½½åœºæ™¯", 0.17, 12000),
            ("ä½è´Ÿè½½åœºæ™¯", 0.5, 14000),
            ("ä¸­ä½è´Ÿè½½åœºæ™¯", 0.8, 16000),
            ("ä¸­è´Ÿè½½åœºæ™¯(å°æ‰¹é‡)", 1.0, 18000),
            ("ä¸­è´Ÿè½½åœºæ™¯(å¤§æ‰¹é‡)", 1.2, 20000),
            ("ä¸­é«˜è´Ÿè½½åœºæ™¯", 1.4, 22000),
            ("è¾ƒé«˜è´Ÿè½½åœºæ™¯", 1.5, 25000),
            ("é«˜è´Ÿè½½", 1.6, 30000),
            ("æ¥è¿‘é¥±å’Œè´Ÿè½½", 1.69, 38000),
            ("å³°å€¼/é¥±å’Œè´Ÿè½½", 1.78, 48000),
        ]

        print(f"\nğŸ¯ ç”Ÿæˆæµ‹è¯•åœºæ™¯ (åŸºäºæœ€ä½å»¶è¿Ÿ{min_latency:.0f}ms):")
        for name, rate, slo in scenarios:
            print(f"   {name}: {rate}req/s, SLOâ‰¤{slo:.0f}ms")

        return scenarios

    def _analyze_comparison(
            self,
            ds2_result: Optional["Config"],
            conttune_result: Optional["Config"],
            infertuner_result: Optional["Config"],
            scenario_name: str
    ) -> Tuple[str, Optional[float], Optional[float]]:
        """
        åˆ†æå¯¹æ¯”ç»“æœ:
        è¿”å›ï¼š
        - æœ€ä¼˜ç®—æ³•åå­—
        - InferTuner ç›¸å¯¹äº DS2 çš„ GPU èŠ‚çœ (è‹¥ DS2 æ— è§£åˆ™ä¸º None)
        - InferTuner ç›¸å¯¹äº ContTune çš„ GPU èŠ‚çœ (è‹¥ ContTune æ— è§£æˆ– InferTuner æ— è§£åˆ™ä¸º None)
        """

        results = {
            "DS2": ds2_result,
            "ContTune": conttune_result,
            "InferTuner": infertuner_result
        }

        for name, res in results.items():
            if res:
                print(f"{name}: p={res.p}, b={res.b} â†’ cost={res.cost}, latency={res.predicted_latency}, throughput={res.predicted_throughput}")
            else:
                print(f"{name}: æ— è§£")

        # é€‰æ‹© cost æœ€å°çš„ç®—æ³•
        valid_results = {name: res for name, res in results.items() if res is not None}
        if not valid_results:
            print("âŒ æ‰€æœ‰ç®—æ³•æ— è§£")
            return "None", None, None

        best_name = min(valid_results, key=lambda k: valid_results[k].cost)
        print(f"âœ… æœ€ä¼˜ç®—æ³•ï¼ˆæŒ‰costï¼‰: {best_name}")

        # è®¡ç®— GPU èŠ‚çœï¼ˆInferTuner vs DS2ï¼‰
        savings_vs_ds2 = None
        if ds2_result and infertuner_result:
            savings_vs_ds2 = ds2_result.cost - infertuner_result.cost

        # è®¡ç®— GPU èŠ‚çœï¼ˆInferTuner vs ContTuneï¼‰
        savings_vs_conttune = None
        if conttune_result and infertuner_result:
            savings_vs_conttune = conttune_result.cost - infertuner_result.cost

        return best_name, savings_vs_ds2, savings_vs_conttune

    def compare_scenario(self, scenario_name: str, target_rate: float, target_slo: float):
        """å¯¹æ¯”å•ä¸ªåœºæ™¯ï¼Œå¹¶è¿”å›è¯¦ç»†è®°å½•"""
        print(f"\n" + "=" * 70)
        print(f"ğŸ“Š åœºæ™¯: {scenario_name}")
        print("=" * 70)

        # è¿è¡Œä¸‰ç§ç®—æ³•
        ds2_result = self.ds2.ds2_scaling_decision(target_rate, target_slo)

        conttune = ContTuneAlgorithm(
            measure_fn=self.measure_fn,
            target_throughput=target_rate,
            slo=target_slo,
            performance_data=self.df,
            max_parallelism=19,
            min_parallelism=1,
            big_multiplier=1.25,
            small_max_iters=3,
            history_max_len=10
        )
        conttune_result = conttune.conttune_scaling_decision(start_parallelism=1)

        infertuner_result = self.infertuner.infertuner_scaling_decision(target_rate, target_slo)

        # è®¡ç®—æœ€ä¼˜ç®—æ³• + èŠ‚çœæƒ…å†µ
        best_name, savings_vs_ds2, savings_vs_conttune = self._analyze_comparison(
            ds2_result=ds2_result,
            conttune_result=conttune_result,
            infertuner_result=infertuner_result,
            scenario_name=scenario_name
        )

        # æå–ç®—æ³•é…ç½®
        def extract_info(result: Optional["Config"]):
            if result:
                return result.p, result.b, result.cost, result.predicted_throughput, result.predicted_latency
            return None, None, None, None, None

        ds2_p, ds2_b, ds2_cost, ds2_tp, ds2_lat = extract_info(ds2_result)
        cont_p, cont_b, cont_cost, cont_tp, cont_lat = extract_info(conttune_result)
        inf_p, inf_b, inf_cost, inf_tp, inf_lat = extract_info(infertuner_result)

        record = {
            "Scenario": scenario_name,
            "Target_Throughput(req/s)": target_rate,
            "Target_SLO(ms)": target_slo,

            # DS2
            "DS2_p": ds2_p,
            "DS2_b": ds2_b,
            "DS2_cost": ds2_cost,
            "DS2_throughput": ds2_tp,
            "DS2_latency(ms)": ds2_lat,

            # ContTune
            "ContTune_p": cont_p,
            "ContTune_b": cont_b,
            "ContTune_cost": cont_cost,
            "ContTune_throughput": cont_tp,
            "ContTune_latency(ms)": cont_lat,

            # InferTuner
            "InferTuner_p": inf_p,
            "InferTuner_b": inf_b,
            "InferTuner_cost": inf_cost,
            "InferTuner_throughput": inf_tp,
            "InferTuner_latency(ms)": inf_lat,

            # å¯¹æ¯”ç»“æœ
            "Best_Algorithm": best_name,
            "InferTuner_vs_DS2_Savings": savings_vs_ds2,
            "InferTuner_vs_ContTune_Savings": savings_vs_conttune
        }

        return record

    def run_complete_comparison(self, output_csv="comparison_results.csv"):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”ï¼Œå¹¶å°†ç»“æœä¿å­˜ä¸ºCSV"""
        print(f"\nğŸš€ å¼€å§‹ DS2 vs ContTune vs InferTuner å®Œæ•´å¯¹æ¯”")

        # ç”Ÿæˆæµ‹è¯•åœºæ™¯
        scenarios = self.generate_realistic_scenarios()

        # æ‰§è¡Œå¯¹æ¯”
        all_records = []
        total_savings_ds2 = 0
        total_savings_conttune = 0

        for name, rate, slo in scenarios:
            record = self.compare_scenario(name, rate, slo)
            all_records.append(record)

            if record["InferTuner_vs_DS2_Savings"]:
                total_savings_ds2 += record["InferTuner_vs_DS2_Savings"]
            if record["InferTuner_vs_ContTune_Savings"]:
                total_savings_conttune += record["InferTuner_vs_ContTune_Savings"]

        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(all_records)
        df.to_csv(output_csv, index=False)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ° {output_csv}")
        print(f"ğŸ¯ æ€»GPUèŠ‚çœ (InferTuner vs DS2): {total_savings_ds2}")
        print(f"ğŸ¯ æ€»GPUèŠ‚çœ (InferTuner vs ContTune): {total_savings_conttune}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python3 ds2_vs_infertuner.py <performance_data.csv>")
        print("ä¾‹å¦‚: python3 ds2_vs_infertuner.py data/performance_profiling/performance_matrix_20250817_131935.csv")
        sys.exit(1)

    data_file = sys.argv[1]

    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        sys.exit(1)

    print("ğŸ¯ DS2 vs ContTune vs InferTuner è®ºæ–‡æ–¹æ³•å®Œæ•´å®ç°ä¸éªŒè¯")
    print("=" * 60)

    try:
        # åˆ›å»ºå¯¹æ¯”å™¨å¹¶è¿è¡ŒéªŒè¯
        comparator = AlgorithmComparator(data_file)
        comparator.run_complete_comparison()

    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
