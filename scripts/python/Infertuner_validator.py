"""
DS2 vs InferTuner
DS2: åŸºäºçœŸå®å¤„ç†ç‡çš„å¹¶è¡Œåº¦ä¼˜åŒ– (OSDI'18)
InferTuner: è”åˆä¼˜åŒ–å¹¶è¡Œåº¦å’Œæ‰¹å¤§å°çš„åŠ¨æ€è§„åˆ’æ–¹æ³•ï¼ˆä½¿ç”¨çœŸå®æ€§èƒ½æ•°æ®ä»£æ›¿ï¼‰

ä½¿ç”¨æ–¹æ³•:
python3 infertuner_validator.py ../data/performance_profiling/performance_matrix_20250817_131935.csv

"""

import os
import sys
import math
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Tuple, Optional, List, Dict
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

@dataclass
class Config:
    """é…ç½®ç»“æœ"""
    p: int  # å¹¶è¡Œåº¦ï¼ˆGPUæ•°é‡ï¼‰
    b: int  # æ‰¹å¤§å°
    cost: float  # æˆæœ¬ï¼ˆGPUæ•°é‡ï¼‰
    predicted_latency: float
    predicted_throughput: float

class PerformanceModel:
    """æ€§èƒ½é¢„æµ‹æ¨¡å‹"""

    def __init__(self, performance_data: pd.DataFrame):
        self.df = performance_data
        self.latency_model = None
        self.throughput_model = None
        self._build_models()

    def _build_models(self):
        """æ„å»ºæœºå™¨å­¦ä¹ æ€§èƒ½é¢„æµ‹æ¨¡å‹"""
        print("ğŸ§  æ„å»ºæ€§èƒ½é¢„æµ‹æ¨¡å‹...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.df[['p', 'b', 'target_rate']].values
        y_latency = self.df['avg_latency'].values
        y_throughput = self.df['actual_throughput'].values

        # è®­ç»ƒå»¶è¿Ÿé¢„æµ‹æ¨¡å‹
        self.latency_model = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42
        )
        self.latency_model.fit(X, y_latency)

        # è®­ç»ƒååé‡é¢„æµ‹æ¨¡å‹
        self.throughput_model = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42
        )
        self.throughput_model.fit(X, y_throughput)

        # è¯„ä¼°æ¨¡å‹ç²¾åº¦
        X_train, X_test, y_lat_train, y_lat_test, y_thr_train, y_thr_test = train_test_split(
            X, y_latency, y_throughput, test_size=0.2, random_state=42
        )

        lat_pred = self.latency_model.predict(X_test)
        thr_pred = self.throughput_model.predict(X_test)

        lat_mae = mean_absolute_error(y_lat_test, lat_pred)
        lat_mape = mean_absolute_percentage_error(y_lat_test, lat_pred) * 100
        thr_mae = mean_absolute_error(y_thr_test, thr_pred)
        thr_mape = mean_absolute_percentage_error(y_thr_test, thr_pred) * 100

        print(f"   å»¶è¿Ÿæ¨¡å‹: MAE={lat_mae:.1f}ms, MAPE={lat_mape:.1f}%")
        print(f"   ååé‡æ¨¡å‹: MAE={thr_mae:.2f}req/s, MAPE={thr_mape:.1f}%")

    def predict(self, p: int, b: int, target_rate: float) -> Tuple[float, float]:
        """é¢„æµ‹ç»™å®šé…ç½®çš„æ€§èƒ½"""
        X = np.array([[p, b, target_rate]])
        latency = self.latency_model.predict(X)[0]
        throughput = self.throughput_model.predict(X)[0]
        return latency, throughput

class DS2Algorithm:
    """
    DS2ç®—æ³•å®ç° (OSDI'18)
    æ ¸å¿ƒæ€æƒ³ï¼šåŸºäºçœŸå®å¤„ç†ç‡ä¼˜åŒ–å¹¶è¡Œåº¦ï¼Œæ‰¹å¤§å°å›ºå®šä¸º1
    """

    def __init__(self, performance_data: pd.DataFrame):
        self.df = performance_data
        self.performance_model = PerformanceModel(performance_data)
        self._build_true_rate_model()

    def _build_true_rate_model(self):
        """æ„å»ºDS2çš„çœŸå®å¤„ç†ç‡æ¨¡å‹"""
        print("ğŸ”„ æ„å»ºDS2çœŸå®å¤„ç†ç‡æ¨¡å‹...")

        # DS2åªä½¿ç”¨batch_size=1çš„æ•°æ®
        df_b1 = self.df[self.df['b'] == 1].copy()

        if len(df_b1) == 0:
            raise ValueError("DS2éœ€è¦batch_size=1çš„æ€§èƒ½æ•°æ®")

        # è®¡ç®—æ¯ä¸ªå¹¶è¡Œåº¦çš„å•å®ä¾‹çœŸå®å¤„ç†ç‡
        self.true_rate_per_instance = {}
        for p in df_b1['p'].unique():
            p_data = df_b1[df_b1['p'] == p]
            # DS2å‡è®¾ï¼šåœ¨æ— backpressureæ—¶ï¼Œå®é™…ååé‡å°±æ˜¯çœŸå®å¤„ç†ç‡
            avg_throughput = p_data['actual_throughput'].mean()
            # å•å®ä¾‹çš„å®é™…ååé‡ï¼ˆçœŸå®å¤„ç†ç‡ï¼‰
            single_instance_rate = avg_throughput / p
            self.true_rate_per_instance[p] = single_instance_rate
            print(f"   p={p}: å•å®ä¾‹çœŸå®å¤„ç†ç‡={single_instance_rate:.3f} req/s")

    def estimate_true_processing_rate(self, parallelism: int) -> float:
        """
        DS2æ ¸å¿ƒï¼šä¼°ç®—çœŸå®å¤„ç†ç‡
        å‡è®¾çº¿æ€§æ‰©å±•ï¼štotal_rate = single_instance_rate * parallelism
        """

        # å¦‚æœæ˜¯å½“å‰å·²çŸ¥çš„å¹¶è¡Œåº¦ï¼Œç›´æ¥è¿”å›çº¿æ€§æ‰©å±•çš„çœŸæ˜¯å¤„ç†ç‡
        if parallelism in self.true_rate_per_instance:
            return self.true_rate_per_instance[parallelism] * parallelism

        # å¦åˆ™ï¼Œè¿›è¡Œçº¿æ€§æ’å€¼ä¼°ç®—
        # è·å–å·²çŸ¥çš„å¹¶è¡Œåº¦åˆ—è¡¨
        known_p = list(self.true_rate_per_instance.keys())
        # å¦‚æœå½“å‰è¿˜æ²¡æœ‰æ€§èƒ½æ•°æ®
        if not known_p:
            return 1.0

        # æ–¹æ¡ˆ1.çº¿æ€§æ’å€¼
#         known_rates = [self.true_rate_per_instance[p] for p in known_p]
#         estimated_single_rate = np.interp(parallelism, known_p, known_rates)
#         return estimated_single_rate * parallelism

        # æ–¹æ¡ˆ2.ä½¿ç”¨æœ€æ¥è¿‘çš„é…ç½®
        closest_p = min(known_p, key=lambda x: abs(x - parallelism))
        single_rate = self.true_rate_per_instance[closest_p]
        return single_rate * parallelism

    def ds2_scaling_decision(self, target_rate: float, target_slo: float) -> Optional[Config]:
        """
        DS2æ ¸å¿ƒç®—æ³•ï¼šåŸºäºçœŸå®å¤„ç†ç‡è®¡ç®—æœ€ä¼˜å¹¶è¡Œåº¦

        DS2å…¬å¼ï¼šoptimal_parallelism = target_rate / true_rate_per_instance
        çº¦æŸï¼šfixed batch_size = 1
        """
        print(f"\nğŸ”„ DS2ç®—æ³•")
        print(f"   ç›®æ ‡: {target_rate} req/s, SLO â‰¤ {target_slo}ms")
        print(f"   çº¦æŸ: æ‰¹å¤§å°å›ºå®š b=1")

        feasible_configs = []
        batch_size = 1  # DS2æ ¸å¿ƒçº¦æŸ

        # DS2æœç´¢ç©ºé—´ï¼šåªèƒ½è°ƒæ•´å¹¶è¡Œåº¦
        available_p = sorted(self.true_rate_per_instance.keys())
        max_p = max(available_p) if available_p else 4

        for p in range(1, max_p + 1):
            # DS2æ ¸å¿ƒï¼šåŸºäºçœŸå®å¤„ç†ç‡åˆ¤æ–­
            true_processing_rate = self.estimate_true_processing_rate(p)

            # DS2çº¦æŸæ£€æŸ¥
            throughput_ok = true_processing_rate >= target_rate * 0.95  # 5%å®¹å·®

            if not throughput_ok:
                print(f"   âŒ p={p} ä¸æ»¡è¶³ååé‡çº¦æŸ: çœŸå®å¤„ç†ç‡={true_processing_rate:.2f}req/s < {target_rate * 0.95:.2f}req/s")
                continue

            # ä½¿ç”¨æ€§èƒ½æ¨¡å‹ä¼°ç®—å»¶è¿Ÿ
            pred_latency, _ = self.performance_model.predict(p, batch_size, target_rate)
            latency_ok = pred_latency <= target_slo

            if not latency_ok:
                print(f"   âŒ p={p} ä¸æ»¡è¶³å»¶è¿Ÿçº¦æŸ: é¢„æµ‹å»¶è¿Ÿ={pred_latency:.0f}ms > {target_slo:.0f}ms")
                continue

            cost = p  # GPUæ•°é‡
            config = Config(p, batch_size, cost, pred_latency, true_processing_rate)
            feasible_configs.append(config)
            print(f"   âœ… å¯è¡Œ: p={p}, b={batch_size}, æˆæœ¬={cost}GPU, "
                  f"çœŸå®å¤„ç†ç‡={true_processing_rate:.2f}req/s, å»¶è¿Ÿâ‰ˆ{pred_latency:.0f}ms")

        if not feasible_configs:
            print(f"   âŒ DS2æ— å¯è¡Œé…ç½®")
            return None

        # DS2é€‰æ‹©æœ€å°å¹¶è¡Œåº¦ï¼ˆæœ€å°æˆæœ¬ï¼‰
        best_config = min(feasible_configs, key=lambda x: x.cost)
        print(f"   ğŸ† DS2æœ€ä¼˜: p={best_config.p}, b={best_config.b}, æˆæœ¬={best_config.cost}GPU")

        return best_config

class InferTunerAlgorithm:
    """
    InferTunerç®—æ³•å®ç°
    æ ¸å¿ƒæ€æƒ³ï¼šè”åˆä¼˜åŒ–å¹¶è¡Œåº¦å’Œæ‰¹å¤§å°ï¼Œä½¿ç”¨åŠ¨æ€è§„åˆ’æ±‚è§£
    """

    def __init__(self, performance_data: pd.DataFrame):
        self.df = performance_data
        self.performance_model = PerformanceModel(performance_data)
        # æŒ‰é…ç½®åˆ†ç»„å–å¹³å‡å€¼
        self.df_avg = self.df.groupby(['p', 'b', 'target_rate']).agg({
            'actual_throughput': 'mean',
            'avg_latency': 'mean'
        }).reset_index()

    def generate_feasible_configs(self, target_rate: float, target_slo: float) -> List[Config]:
        """ç”Ÿæˆæ‰€æœ‰å¯è¡Œçš„(p,b)é…ç½®"""
        feasible_configs = []

        # InferTuneræœç´¢ç©ºé—´ï¼šè”åˆä¼˜åŒ–(p,b)
        p_values = sorted(self.df_avg['p'].unique())
        b_values = sorted(self.df_avg['b'].unique())

        for p in p_values:
            for b in b_values:
                # ä½¿ç”¨æ€§èƒ½æ¨¡å‹é¢„æµ‹
                pred_latency, pred_throughput = self.performance_model.predict(p, b, target_rate)

                # çº¦æŸæ£€æŸ¥
                throughput_ok = pred_throughput >= target_rate * 0.95  # 5%å®¹å·®
                if not throughput_ok:
                    print(f"   âŒ p={p} ä¸æ»¡è¶³ååé‡çº¦æŸ: é¢„æµ‹å¤„ç†ç‡={pred_throughput:.2f}req/s < {target_rate * 0.95:.2f}req/s")
                    continue

                latency_ok = pred_latency <= target_slo
                if not latency_ok:
                    print(f"   âŒ p={p} ä¸æ»¡è¶³å»¶è¿Ÿçº¦æŸ: é¢„æµ‹å»¶è¿Ÿ={pred_latency:.0f}ms > {target_slo:.0f}ms")
                    continue

                cost = p  # GPUæ•°é‡ä½œä¸ºæˆæœ¬
                config = Config(p, b, cost, pred_latency, pred_throughput)
                feasible_configs.append(config)

        return feasible_configs

    def infertuner_scaling_decision(self, target_rate: float, target_slo: float) -> Optional[Config]:
        """
        InferTuneræ ¸å¿ƒç®—æ³•ï¼šè”åˆä¼˜åŒ–(p,b)
        åŠ¨æ€è§„åˆ’ï¼šåœ¨å¯è¡Œé…ç½®ä¸­é€‰æ‹©æœ€å°æˆæœ¬
        """
        print(f"\nğŸ¯ InferTunerç®—æ³•")
        print(f"   ç›®æ ‡: {target_rate} req/s, SLO â‰¤ {target_slo}ms")
        print(f"   ä¼˜åŠ¿: è”åˆä¼˜åŒ–å¹¶è¡Œåº¦(p)å’Œæ‰¹å¤§å°(b)")

        # ç”Ÿæˆå¯è¡Œé…ç½®
        feasible_configs = self.generate_feasible_configs(target_rate, target_slo)

        if not feasible_configs:
            print(f"   âŒ InferTuneræ— å¯è¡Œé…ç½®")
            return None

        # æ˜¾ç¤ºå¯è¡Œé…ç½®
        print(f"   æœç´¢ç©ºé—´: pâˆˆ{sorted(self.df_avg['p'].unique())}, bâˆˆ{sorted(self.df_avg['b'].unique())}")
        for config in feasible_configs:
            print(f"   âœ… å¯è¡Œ: p={config.p}, b={config.b}, æˆæœ¬={config.cost}GPU, "
                  f"ååé‡â‰ˆ{config.predicted_throughput:.2f}req/s, å»¶è¿Ÿâ‰ˆ{config.predicted_latency:.0f}ms")

        # InferTuneré€‰æ‹©æœ€å°æˆæœ¬é…ç½®
        best_config = min(feasible_configs, key=lambda x: x.cost)
        print(f"   ğŸ† InferTuneræœ€ä¼˜: p={best_config.p}, b={best_config.b}, æˆæœ¬={best_config.cost}GPU")

        return best_config

class AlgorithmComparator:
    """ç®—æ³•å¯¹æ¯”å™¨"""

    def __init__(self, performance_data_file: str):
        # åŠ è½½æ•°æ®
        self.df = pd.read_csv(performance_data_file)
        print(f"ğŸ“Š åŠ è½½æ€§èƒ½æ•°æ®: {len(self.df)} æ¡è®°å½•")

        # æ•°æ®æ¸…æ´—
        self.df = self.df[
            (self.df['actual_throughput'] > 0) &
            (self.df['avg_latency'] > 0) &
            (self.df['success_rate'] > 90)
        ].copy()
        print(f"   æ¸…æ´—å: {len(self.df)} æ¡æœ‰æ•ˆè®°å½•")

        # åˆå§‹åŒ–ç®—æ³•
        self.ds2 = DS2Algorithm(self.df)
        self.infertuner = InferTunerAlgorithm(self.df)

        # æ˜¾ç¤ºæ•°æ®èŒƒå›´
        self._show_data_summary()

    def _show_data_summary(self):
        """æ˜¾ç¤ºæ•°æ®æ¦‚å†µ"""
        print(f"\nğŸ“ˆ æ•°æ®æ¦‚å†µ:")
        print(f"   å¹¶è¡Œåº¦èŒƒå›´: {self.df['p'].min()}-{self.df['p'].max()}")
        print(f"   æ‰¹å¤§å°èŒƒå›´: {self.df['b'].min()}-{self.df['b'].max()}")
        print(f"   è¯·æ±‚é€Ÿç‡èŒƒå›´: {self.df['target_rate'].min():.1f}-{self.df['target_rate'].max():.1f} req/s")
        print(f"   å»¶è¿ŸèŒƒå›´: {self.df['avg_latency'].min():.0f}-{self.df['avg_latency'].max():.0f} ms")

    def generate_realistic_scenarios(self) -> List[Tuple[str, float, float]]:
        """åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæµ‹è¯•åœºæ™¯"""
        min_latency = self.df['avg_latency'].min()

        scenarios = [
            ("ä½è´Ÿè½½åœºæ™¯", 1.0, min_latency + 500),
            ("ä¸­è´Ÿè½½åœºæ™¯", 2.0, min_latency + 800),
            ("é«˜è´Ÿè½½åœºæ™¯", 3.5, min_latency + 1200),
            ("ä¸¥æ ¼SLOåœºæ™¯", 1.5, min_latency + 300),
        ]

        print(f"\nğŸ¯ ç”Ÿæˆæµ‹è¯•åœºæ™¯ (åŸºäºæœ€ä½å»¶è¿Ÿ{min_latency:.0f}ms):")
        for name, rate, slo in scenarios:
            print(f"   {name}: {rate}req/s, SLOâ‰¤{slo:.0f}ms")

        return scenarios

    def compare_scenario(self, scenario_name: str, target_rate: float, target_slo: float):
        """å¯¹æ¯”å•ä¸ªåœºæ™¯"""
        print(f"\n" + "="*70)
        print(f"ğŸ“Š åœºæ™¯: {scenario_name}")
        print("="*70)

        # è¿è¡Œä¸¤ç§ç®—æ³•
        ds2_result = self.ds2.ds2_scaling_decision(target_rate, target_slo)
        infertuner_result = self.infertuner.infertuner_scaling_decision(target_rate, target_slo)

        # å¯¹æ¯”åˆ†æ
        return self._analyze_comparison(ds2_result, infertuner_result, scenario_name)

    def _analyze_comparison(self, ds2_result: Optional[Config],
                          infertuner_result: Optional[Config],
                          scenario_name: str) -> Tuple[str, float]:
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        print(f"\nğŸ“‹ ç®—æ³•å¯¹æ¯”:")

        if ds2_result and infertuner_result:
            print(f"DS2:       p={ds2_result.p}, b={ds2_result.b} â†’ {ds2_result.cost}GPU")
            print(f"InferTuner: p={infertuner_result.p}, b={infertuner_result.b} â†’ {infertuner_result.cost}GPU")

            if infertuner_result.cost < ds2_result.cost:
                savings = ds2_result.cost - infertuner_result.cost
                print(f"InferTuner: èŠ‚çœ{savings}GPU")
                return "InferTuner", savings
            elif infertuner_result.cost == ds2_result.cost:
                print(f"æˆæœ¬ç›¸åŒ")
                return "Tie", 0
            else:
                print(f"DS2æ›´ä¼˜")
                return "DS2", 0

        elif infertuner_result and not ds2_result:
            print(f"åªæœ‰InferTuneræ‰¾åˆ°è§£: p={infertuner_result.p}, b={infertuner_result.b}")
            return "InferTuner", 0
        elif ds2_result and not infertuner_result:
            print(f"åªæœ‰DS2æ‰¾åˆ°è§£: p={ds2_result.p}, b={ds2_result.b}")
            return "DS2", 0
        else:
            print(f"éƒ½æ— è§£")
            return "None", 0

    def run_complete_comparison(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”"""
        print(f"\nğŸš€ å¼€å§‹ DS2 vs InferTuner å®Œæ•´å¯¹æ¯”")

        # ç”Ÿæˆæµ‹è¯•åœºæ™¯
        scenarios = self.generate_realistic_scenarios()

        # æ‰§è¡Œå¯¹æ¯”
        results = []
        total_savings = 0

        for name, rate, slo in scenarios:
            winner, savings = self.compare_scenario(name, rate, slo)
            results.append((name, winner, savings))
            total_savings += savings

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python3 ds2_vs_infertuner.py <performance_data.csv>")
        print("ä¾‹å¦‚: python3 ds2_vs_infertuner.py data/performance_profiling/performance_matrix_20250817_131935.csv")
        sys.exit(1)

    data_file = sys.argv[1]

    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        sys.exit(1)

    print("ğŸ¯ DS2 vs InferTuner è®ºæ–‡æ–¹æ³•å®Œæ•´å®ç°ä¸éªŒè¯")
    print("="*60)

    try:
        # åˆ›å»ºå¯¹æ¯”å™¨å¹¶è¿è¡ŒéªŒè¯
        comparator = AlgorithmComparator(data_file)
        comparator.run_complete_comparison()

    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()