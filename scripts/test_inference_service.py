#!/usr/bin/env python3
"""
æµ‹è¯•æ¨ç†æœåŠ¡çš„åŠŸèƒ½
"""

import subprocess
import json
import time
import threading
import queue

def test_inference_service():
    model_path = "/workspace/models/Qwen1.5-1.8B-Chat"
    service_script = "/workspace/infertuner-simple/scripts/simple_inference_service.py"
    
    print("ğŸ§ª å¯åŠ¨æ¨ç†æœåŠ¡æµ‹è¯•...")
    
    # å¯åŠ¨æ¨ç†æœåŠ¡è¿›ç¨‹
    try:
        process = subprocess.Popen(
            ["python3", service_script, model_path],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1
        )
        
        print("âœ… æ¨ç†æœåŠ¡è¿›ç¨‹å¯åŠ¨")
        
        # ç­‰å¾…æœåŠ¡åˆå§‹åŒ–
        print("â³ ç­‰å¾…æœåŠ¡åˆå§‹åŒ–...")
        time.sleep(8)
        
        # æµ‹è¯•è¯·æ±‚
        test_requests = [
            {
                "user_message": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ ",
                "max_tokens": 50,
                "request_id": "test_001"
            },
            {
                "user_message": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                "max_tokens": 60,
                "request_id": "test_002"
            },
            {
                "user_message": "èƒ½è§£é‡Šä¸€ä¸‹transformerå—ï¼Ÿ",
                "max_tokens": 80,
                "request_id": "test_003"
            }
        ]
        
        print(f"ğŸ“¤ å‘é€ {len(test_requests)} ä¸ªæµ‹è¯•è¯·æ±‚...\n")
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        for i, req in enumerate(test_requests, 1):
            print(f"--- æµ‹è¯•è¯·æ±‚ {i}/{len(test_requests)} ---")
            print(f"å‘é€: {req['user_message']}")
            
            # å‘é€è¯·æ±‚
            request_json = json.dumps(req) + "\n"
            process.stdin.write(request_json)
            process.stdin.flush()
            
            # è¯»å–å“åº”
            response_line = process.stdout.readline()
            if response_line:
                try:
                    response = json.loads(response_line.strip())
                    
                    if response.get("success"):
                        print(f"âœ… æˆåŠŸ: {response['response'][:100]}...")
                        print(f"æ¨ç†æ—¶é—´: {response['inference_time_ms']:.2f}ms")
                    else:
                        print(f"âŒ å¤±è´¥: {response.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        
                except json.JSONDecodeError as e:
                    print(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
                    print(f"åŸå§‹å“åº”: {response_line}")
            else:
                print("âŒ æœªæ”¶åˆ°å“åº”")
            
            print()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“Š è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯...")
        stats_request = json.dumps({"command": "stats"}) + "\n"
        process.stdin.write(stats_request)
        process.stdin.flush()
        
        stats_response = process.stdout.readline()
        if stats_response:
            try:
                stats_data = json.loads(stats_response.strip())
                print("ç»Ÿè®¡ä¿¡æ¯:")
                for key, value in stats_data.get("stats", {}).items():
                    print(f"  {key}: {value}")
            except json.JSONDecodeError:
                print(f"ç»Ÿè®¡ä¿¡æ¯è§£æå¤±è´¥: {stats_response}")
        
        # å…³é—­æœåŠ¡
        print("\nğŸ›‘ å…³é—­æ¨ç†æœåŠ¡...")
        shutdown_request = json.dumps({"command": "shutdown"}) + "\n"
        process.stdin.write(shutdown_request)
        process.stdin.flush()
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        process.wait(timeout=10)
        print("âœ… æ¨ç†æœåŠ¡æµ‹è¯•å®Œæˆ")
        
    except subprocess.TimeoutExpired:
        print("âš ï¸ æœåŠ¡å…³é—­è¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
        process.kill()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        if process.poll() is None:
            process.terminate()
        raise

if __name__ == "__main__":
    test_inference_service()
