#!/opt/conda/envs/vllm-env/bin/python3

import torch
import json
import sys
import time
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

# é…ç½®æ—¥å¿—è¾“å‡ºåˆ°stderrï¼Œé¿å…ä¸stdoutæ•°æ®æµå†²çª
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger(__name__)

class SimpleInferenceService:
    """ç²¾ç®€ç‰ˆæ¨ç†æœåŠ¡ - GPUåŠ é€Ÿç‰ˆæœ¬"""
    
    def __init__(self, node_ip: str, model_path: str, gpu_id: int = 0):
        """
            node_ipæŒ‡æ˜æ¨ç†è¿›ç¨‹æ‰€åœ¨èŠ‚ç‚¹çš„IPåœ°å€
        """

        self.node_ip = node_ip
        self.model_path = model_path
        self.gpu_id = gpu_id
        
        # GPUè®¾å¤‡é…ç½®
        if torch.cuda.is_available():
            self.device = torch.device(f'cuda:{gpu_id}')
            logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(gpu_id)}")
        else:
            self.device = torch.device('cpu')
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
        
        # åŠ è½½æ¨¡å‹
        self.tokenizer, self.model = self._load_model()
        
        # ç®€åŒ–ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "total_inference_time": 0.0,
            "errors": 0,
            "start_time": time.time()
        }
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹åˆ°GPU"""
        try:
            logger.info(f"èŠ‚ç‚¹ {self.node_ip} ä» {self.model_path} åŠ è½½æ¨¡å‹åˆ° {self.device}")
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹åˆ°GPU
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,  # GPUä½¿ç”¨float16èŠ‚çœæ˜¾å­˜
                device_map="auto"
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            model.eval()
            
            # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / 1e9
                allocated = torch.cuda.memory_allocated(self.device) / 1e9
                logger.info(f"èŠ‚ç‚¹ {self.node_ip} çš„GPUæ˜¾å­˜: {allocated:.1f}GB / {gpu_memory:.1f}GB")
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return tokenizer, model
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_response(self, request_data):
        """ç”Ÿæˆå›å¤"""
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # è§£æè¯·æ±‚å‚æ•°
            user_message = request_data.get('user_message', '')
            max_tokens = request_data.get('max_tokens', 640)
            request_id = request_data.get('request_id', 'unknown')
            batch_size = request_data.get('batch_size', 1)  # æ”¯æŒæ‰¹å¤„ç†å‚æ•°
            
            if not user_message:
                raise ValueError("user_messageä¸èƒ½ä¸ºç©º")
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [{"role": "user", "content": user_message}]
            
            # ç”Ÿæˆå›å¤
            response_text = self._generate_text(messages, max_tokens)
            
            # è®¡ç®—æ¨ç†æ—¶é—´
            inference_time = (time.time() - start_time) * 1000
            self.stats["total_inference_time"] += inference_time
            
            # æ„å»ºå“åº”
            result = {
                "response": response_text,
                "inference_time_ms": round(inference_time, 2),
                "model_name": "Qwen1.5-1.8B-Chat",
                "request_id": request_id,
                "batch_size": batch_size,
                "success": True,
                "timestamp": int(time.time() * 1000)
            }
            
            return result
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"æ¨ç†å¤±è´¥ - request_id: {request_data.get('request_id', 'unknown')}, error: {e}")
            
            return {
                "error": str(e),
                "response": "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯",
                "inference_time_ms": 0.0,
                "model_name": "Error",
                "request_id": request_data.get('request_id', 'unknown'),
                "batch_size": request_data.get('batch_size', 1),
                "success": False,
                "timestamp": int(time.time() * 1000)
            }
    
    def _generate_text(self, messages, max_tokens):
        """ç”Ÿæˆæ–‡æœ¬å›å¤"""
        try:
            # åº”ç”¨chatæ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥åˆ°GPU
            inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": max_tokens,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "pad_token_id": self.tokenizer.eos_token_id,
            }
            
            # æ‰§è¡Œç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)
            
            # è§£ç å›å¤ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def get_stats(self):
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        uptime = current_time - self.stats["start_time"]
        
        avg_inference_time = (
            self.stats["total_inference_time"] / self.stats["total_requests"]
            if self.stats["total_requests"] > 0 else 0.0
        )
        
        throughput = (
            self.stats["total_requests"] / uptime
            if uptime > 0 else 0.0
        )
        
        return {
            "total_requests": self.stats["total_requests"],
            "avg_inference_time_ms": round(avg_inference_time, 2),
            "total_inference_time_ms": round(self.stats["total_inference_time"], 2),
            "errors": self.stats["errors"],
            "error_rate": round(self.stats["errors"] / max(self.stats["total_requests"], 1), 3),
            "uptime_seconds": round(uptime, 1),
            "throughput_qps": round(throughput, 3),
            "device": str(self.device),
            "gpu_memory_allocated_gb": torch.cuda.memory_allocated(self.device) / 1e9 if torch.cuda.is_available() else 0,
            "gpu_memory_cached_gb": torch.cuda.memory_reserved(self.device) / 1e9 if torch.cuda.is_available() else 0
        }

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 3:
        logger.error("ç”¨æ³•: python3 simple_inference_service.py <node_ip> <model_path> <gpu_id>")
        sys.exit(1)

    node_ip = sys.argv[1]
    model_path = sys.argv[2]
    gpu_id = int(sys.argv[3]) if len(sys.argv) > 3 else 0
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not Path(model_path).exists():
        logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        logger.error("CUDAä¸å¯ç”¨ï¼è¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAå®‰è£…")
        sys.exit(1)
    
    if gpu_id >= torch.cuda.device_count():
        logger.error(f"GPU {gpu_id} ä¸å­˜åœ¨ï¼å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        sys.exit(1)
    
    # åˆå§‹åŒ–æœåŠ¡
    try:
        service = SimpleInferenceService(node_ip, model_path, gpu_id)
        logger.info("è¾“å…¥æ ¼å¼: JSONåŒ…å«'user_message', 'max_tokens', 'request_id'ç­‰å­—æ®µ")
        logger.info("ç‰¹æ®Šå‘½ä»¤: {'command': 'stats'} è·å–ç»Ÿè®¡, {'command': 'shutdown'} å…³é—­æœåŠ¡")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)
    
    # å¤„ç†è¯·æ±‚å¾ªç¯
    try:
        # åœ¨æ²¡æœ‰å¤–éƒ¨å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè¯¥å¾ªç¯æ˜¯ä¸€ä¸ªæ— é™å¾ªç¯ï¼Œå› ä¸ºsys.stdinæ˜¯ä¸€ä¸ªæµå¼è¾“å…¥ï¼ŒPython ä¼šæŒç»­è¯»å–ç›´åˆ°æµå…³é—­æˆ–å‘ç”Ÿç‰¹å®šäº‹ä»¶ã€‚
        for line_num, line in enumerate(sys.stdin, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                # è§£æJSONè¯·æ±‚
                request_data = json.loads(line)
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if request_data.get("command") == "stats":
                    stats = service.get_stats()
                    print(json.dumps({"stats": stats}))
                    sys.stdout.flush()
                    continue
                
                if request_data.get("command") == "shutdown":
                    logger.info("æ”¶åˆ°å…³é—­å‘½ä»¤ï¼Œæ­£åœ¨é€€å‡º...")
                    break
                
                # å¤„ç†æ¨ç†è¯·æ±‚
                result = service.generate_response(request_data)
                
                # è¾“å‡ºç»“æœåˆ°stdout
                print(json.dumps(result))
                sys.stdout.flush()
                
                # å®šæœŸè¾“å‡ºç»Ÿè®¡åˆ°stderr
                if line_num % 10 == 0:
                    stats = service.get_stats()
                    logger.info(f"å¤„ç†äº† {line_num} ä¸ªè¯·æ±‚ï¼Œç»Ÿè®¡: {stats}")
                
            except json.JSONDecodeError as e:
                error_result = {
                    "error": f"JSONè§£æé”™è¯¯: {e}",
                    "response": "è¯·æ±‚æ ¼å¼é”™è¯¯",
                    "success": False,
                    "request_id": "json_error",
                    "timestamp": int(time.time() * 1000)
                }
                print(json.dumps(error_result))
                sys.stdout.flush()
                
            except Exception as e:
                logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
                error_result = {
                    "error": f"å¤„ç†é”™è¯¯: {e}",
                    "response": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
                    "success": False,
                    "request_id": "processing_error",
                    "timestamp": int(time.time() * 1000)
                }
                print(json.dumps(error_result))
                sys.stdout.flush()
    
    except KeyboardInterrupt:
        logger.info("æœåŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æœåŠ¡è¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)
    finally:
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        try:
            final_stats = service.get_stats()
            logger.info(f"æœ€ç»ˆç»Ÿè®¡ç»“æœ: {final_stats}")
        except Exception as e:
            logger.error("è·å–æœ€ç»ˆç»Ÿè®¡æ—¶å‡ºé”™")
        finally:
            logger.info("ğŸ›‘ æ¨ç†æœåŠ¡å·²åœæ­¢")

if __name__ == "__main__":
    main()
