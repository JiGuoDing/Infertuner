package com.infertuner.jobs;

import com.infertuner.models.InferenceRequest;
import com.infertuner.models.InferenceResponse;
import com.infertuner.processors.ParallelBatchProcessor;
import com.infertuner.sinks.JointOptimizationSink;
import com.infertuner.sources.BasicRequestSource;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * pÃ—bè”åˆä¼˜åŒ–ä½œä¸š
 * æ ¸å¿ƒåŠŸèƒ½ï¼š
 * 1. æµ‹è¯•ä¸åŒå¹¶è¡Œåº¦(p)å’Œæ‰¹å¤§å°(b)çš„ç»„åˆ
 * 2. æ¯ä¸ªGPUç‹¬ç«‹æ”’æ‰¹ï¼Œå®ç°çœŸæ­£çš„å¹¶è¡Œæ‰¹å¤„ç†
 * 3. ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½çŸ©é˜µç”¨äºè”åˆä¼˜åŒ–
 */
public class JointOptimizationJob {

    private static final Logger logger = LoggerFactory.getLogger(JointOptimizationJob.class);

    public static void main(String[] args) throws Exception {
        logger.info("=== InferTuner pÃ—bè”åˆä¼˜åŒ–éªŒè¯ ===");

        // è§£æå‚æ•°
        int parallelism = args.length > 0 ? Integer.parseInt(args[0]) : 2;        // p: å¹¶è¡Œåº¦/GPUèŠ‚ç‚¹æ•°é‡
        int batchSize = args.length > 1 ? Integer.parseInt(args[1]) : 4;          // b: æ¯æ‰¹å¤§å°
        int batchesPerNode = args.length > 2 ? Integer.parseInt(args[2]) : 6;      // batch_num_per_node æ¯èŠ‚ç‚¹å¤„ç†çš„æ‰¹æ¬¡æ•°
        long interval = args.length > 3 ? Long.parseLong(args[3]) : 200;          // è¯·æ±‚é—´éš”

        // è®¡ç®—æ€»è¯·æ±‚æ•°ï¼šç¡®ä¿æ¯ä¸ªGPUéƒ½èƒ½å¤„ç†è¶³å¤Ÿçš„å®Œæ•´æ‰¹æ¬¡
        int totalRequests = parallelism * batchSize * batchesPerNode;

        logger.info("ğŸ¯ pÃ—bè”åˆä¼˜åŒ–é…ç½®:");
        logger.info("  å¹¶è¡Œåº¦(p): {} â€”â€” GPUèŠ‚ç‚¹ä¸ªæ•°", parallelism);
        logger.info("  æ‰¹å¤§å°(b): {} â€”â€” æ‰¹æ¬¡å†…è¯·æ±‚æ•°", batchSize);
        logger.info("  æ¯èŠ‚ç‚¹å¤„ç†çš„æ€»æ‰¹æ¬¡æ•°: {}", batchesPerNode);
        logger.info("  æ€»è¯·æ±‚æ•°: {} = {}Ã—{}Ã—{}", totalRequests, parallelism, batchSize, batchesPerNode);
        logger.info("  è¯·æ±‚é—´éš”: {}ms", interval);
        logger.info("  é¢„æœŸæ€»æ‰¹æ¬¡: {}", parallelism * batchesPerNode);

        // åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(parallelism);

        // é…ç½®å…¨å±€å‚æ•°
        Configuration config = new Configuration();
        config.setString("batch.size", String.valueOf(batchSize));
        config.setString("parallelism", String.valueOf(parallelism));
        config.setString("total.requests", String.valueOf(totalRequests));
        config.setString("batches.per.gpu", String.valueOf(batchesPerNode));
        env.getConfig().setGlobalJobParameters(config);

        logger.info("âœ… å…¨å±€å‚æ•°é…ç½®å®Œæˆ");

        // ç†è®ºæ€§èƒ½é¢„æµ‹
        // double theoreticalThroughputPerGpu = 1000.0 / (300 + batchSize * 50); // å‡è®¾å•è¯·æ±‚50msæ¨ç†æ—¶é—´
        // double theoreticalTotalThroughput = theoreticalThroughputPerGpu * parallelism;
        // double theoreticalAvgWaitTime = (batchSize - 1) * interval / 2.0; // å¹³å‡ç­‰å¾…æ—¶é—´
        //
        // logger.info("ğŸ“ˆ ç†è®ºæ€§èƒ½é¢„æµ‹:");
        // logger.info("  å•GPUååé‡: {} req/s", theoreticalThroughputPerGpu);
        // logger.info("  æ€»ååé‡: {} req/s", theoreticalTotalThroughput);
        // logger.info("  å¹³å‡ç­‰å¾…æ—¶é—´: {}ms", theoreticalAvgWaitTime);

        DataStream<InferenceRequest> requests = env
                .addSource(new BasicRequestSource(totalRequests, interval))
                .name("Joint Optimization Request Source");

        DataStream<InferenceResponse> responses = requests
                .rebalance()
                .process(new ParallelBatchProcessor())
                .name("Parallel Batch Processor");

        String experimentId = String.format("p%db%d_%dreq", parallelism, batchSize, totalRequests);
        responses.addSink(new JointOptimizationSink(experimentId, parallelism, batchSize, interval))
                .name("Joint Optimization Performance Sink").setParallelism(1);

        logger.info("ğŸš€ pÃ—bè”åˆä¼˜åŒ–æµæ°´çº¿æ„å»ºå®Œæˆ");
        // logger.info("ğŸ“Š æ ¸å¿ƒç‰¹ç‚¹:");
        // logger.info("  1. è¯·æ±‚é€šè¿‡rebalance()è‡ªåŠ¨è½®è¯¢åˆ†å‘åˆ°{}ä¸ªGPU", parallelism);
        // logger.info("  2. æ¯ä¸ªGPUç‹¬ç«‹æ”’æ‰¹{}ä¸ªè¯·æ±‚ï¼ˆä½¿ç”¨å†…å­˜ç¼“å†²ï¼‰", batchSize);
        // logger.info("  3. çœŸå®GPUå¹¶è¡Œå¤„ç†ï¼Œæ— èµ„æºå†²çª");
        // logger.info("  4. ç²¾ç¡®æµ‹é‡ç­‰å¾…æ—¶é—´ã€å¤„ç†æ—¶é—´ã€ååé‡");
        // logger.info("  5. é¿å…Flink Stateå’ŒkeyByçš„å¤æ‚æ€§");

        // logger.info("ğŸ“ˆ é¢„æœŸéªŒè¯æ•ˆæœ:");
        // if (batchSize == 1) {
        //     logger.info("  - b=1: æ— æ”’æ‰¹å¼€é”€ï¼Œå»¶è¿Ÿæœ€ä½ï¼Œä½†ååé‡å—å¯åŠ¨å¼€é”€é™åˆ¶");
        // } else {
        //     logger.info("  - b={}: æ”’æ‰¹åˆ†æ‘Šå¯åŠ¨å¼€é”€ï¼Œæå‡ååé‡ï¼Œä½†å¢åŠ ç­‰å¾…æ—¶é—´", batchSize);
        // }
        //
        // if (parallelism == 1) {
        //     logger.info("  - p=1: å•GPUé™åˆ¶æ€»ååé‡");
        // } else {
        //     logger.info("  - p={}: å¤šGPUå¹¶è¡Œï¼Œååé‡çº¿æ€§æ‰©å±•ï¼ˆç†æƒ³æƒ…å†µï¼‰", parallelism);
        // }

        // æ‰§è¡Œ
        env.execute("InferTuner pÃ—b Joint Optimization Test");

        logger.info("=== pÃ—bè”åˆä¼˜åŒ–éªŒè¯å®Œæˆ ===");
    }
}

/*
 * ğŸ”§ ä½¿ç”¨è¯´æ˜ï¼š
 *
 * 1. ç¼–è¯‘ï¼šmvn clean package
 *
 * 2. å•æ¬¡è¿è¡Œç¤ºä¾‹ï¼š
 * $FLINK_HOME/bin/flink run -c com.infertuner.jobs.JointOptimizationJob \
 *   target/infertuner-1.0.0.jar 2 4 6 200
 *   # å«ä¹‰ï¼š2GPUï¼Œæ‰¹å¤§å°4ï¼Œæ¯GPU 6ä¸ªæ‰¹æ¬¡ï¼Œè¯·æ±‚é—´éš”200ms
 *   # æ€»è¯·æ±‚ï¼š2Ã—4Ã—6=48ä¸ª
 *
 * 3. é…åˆè„šæœ¬è¿›è¡Œå‚æ•°æ‰«æï¼š
 * ./run_joint_optimization.sh
 *
 * 4. å…³é”®éªŒè¯ç‚¹ï¼š
 * - å¹¶è¡Œåº¦æ‰©å±•æ€§ï¼šp1 vs p2 vs p4 çš„ååé‡æå‡
 * - æ‰¹å¤§å°ä¼˜åŒ–ï¼šb1 vs b2 vs b4 vs b8 çš„å»¶è¿Ÿ/ååé‡æƒè¡¡
 * - èµ„æºåˆ©ç”¨ç‡ï¼šé«˜å¹¶è¡Œåº¦ä¸‹çš„GPUåˆ©ç”¨æ•ˆç‡
 * - ç­‰å¾…æ—¶é—´åˆ†å¸ƒï¼šä¸åŒæ‰¹å¤§å°ä¸‹çš„è¯·æ±‚ç­‰å¾…æ—¶é—´
 *
 */