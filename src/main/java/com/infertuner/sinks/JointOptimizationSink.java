package com.infertuner.sinks;

import com.infertuner.models.InferenceResponse;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.ConcurrentHashMap;

/**
 * pÃ—bè”åˆä¼˜åŒ–æ€§èƒ½ç»Ÿè®¡æ±‡èšå™¨
 *
 * æ ¸å¿ƒåŠŸèƒ½ï¼š
 * 1. æ”¶é›†å¹¶è¡Œåº¦(p)å’Œæ‰¹å¤§å°(b)çš„è”åˆæ€§èƒ½æ•°æ®
 * 2. ç»Ÿè®¡æ¯ä¸ªGPUçš„å¤„ç†æƒ…å†µå’Œè´Ÿè½½å‡è¡¡
 * 3. ç”Ÿæˆç”¨äºå‚æ•°ä¼˜åŒ–çš„è¯¦ç»†æŒ‡æ ‡
 */
public class JointOptimizationSink implements SinkFunction<InferenceResponse> {

    private static final Logger logger = LoggerFactory.getLogger(JointOptimizationSink.class);

    private final String experimentId;
    private final int parallelism;
    private final int batchSize;

    // å…¨å±€ç»Ÿè®¡å˜é‡ï¼ˆè·¨æ‰€æœ‰Sinkå®ä¾‹å…±äº«ï¼‰
    private static final AtomicInteger globalTotalRequests = new AtomicInteger(0);
    private static final AtomicInteger globalSuccessRequests = new AtomicInteger(0);
    private static final AtomicLong globalTotalInferenceTime = new AtomicLong(0);
    private static final AtomicLong globalTotalWaitTime = new AtomicLong(0);
    private static final AtomicLong globalTotalLatency = new AtomicLong(0);
    private static final AtomicInteger globalTotalBatches = new AtomicInteger(0);

    // GPUåˆ†å¸ƒç»Ÿè®¡
    private static final ConcurrentHashMap<String, AtomicInteger> gpuRequestCounts = new ConcurrentHashMap<>();
    private static final ConcurrentHashMap<Integer, AtomicInteger> batchSizeDistribution = new ConcurrentHashMap<>();

    // æ—¶é—´è·Ÿè¸ª
    private static volatile long globalStartTime = 0;
    private static volatile long globalFirstResponseTime = 0;
    private static volatile long globalLastResponseTime = 0;
    private static volatile boolean globalFinalStatsOutputted = false;

    // å®ä¾‹å˜é‡
    private final AtomicInteger localRequests = new AtomicInteger(0);
    private final int expectedTotalRequests;

    public JointOptimizationSink(String experimentId, int parallelism, int batchSize) {
        this.experimentId = experimentId;
        this.parallelism = parallelism;
        this.batchSize = batchSize;

        // ä»experimentIdè§£æé¢„æœŸè¯·æ±‚æ•°
        this.expectedTotalRequests = parseExpectedRequestsFromId(experimentId);

        // é‡ç½®å…¨å±€ç»Ÿè®¡
        synchronized (JointOptimizationSink.class) {
            globalTotalRequests.set(0);
            globalSuccessRequests.set(0);
            globalTotalInferenceTime.set(0);
            globalTotalWaitTime.set(0);
            globalTotalLatency.set(0);
            globalTotalBatches.set(0);
            gpuRequestCounts.clear();
            batchSizeDistribution.clear();
            globalStartTime = System.currentTimeMillis();
            globalFirstResponseTime = 0;
            globalLastResponseTime = 0;
            globalFinalStatsOutputted = false;
        }

        logger.info("pÃ—bè”åˆä¼˜åŒ–ç»Ÿè®¡åˆå§‹åŒ–: å®éªŒ={}, p={}, b={}, é¢„æœŸè¯·æ±‚={}",
                experimentId, parallelism, batchSize, expectedTotalRequests);
    }

    private int parseExpectedRequestsFromId(String experimentId) {
        try {
            if (experimentId.contains("req")) {
                String[] parts = experimentId.split("_");
                for (String part : parts) {
                    if (part.endsWith("req")) {
                        return Integer.parseInt(part.replace("req", ""));
                    }
                }
            }
        } catch (Exception e) {
            // é™é»˜å¤„ç†
        }
        return parallelism * batchSize * 6; // é»˜è®¤æ¯GPU 6ä¸ªæ‰¹æ¬¡
    }

    @Override
    public void invoke(InferenceResponse response, Context context) throws Exception {
        long currentTime = System.currentTimeMillis();

        int globalCount = globalTotalRequests.incrementAndGet();
        int localCount = localRequests.incrementAndGet();

        // æ›´æ–°å…¨å±€æ—¶é—´è·Ÿè¸ª
        if (globalFirstResponseTime == 0) {
            synchronized (JointOptimizationSink.class) {
                if (globalFirstResponseTime == 0) {
                    globalFirstResponseTime = currentTime;
                }
            }
        }
        globalLastResponseTime = currentTime;

        // æ›´æ–°å…¨å±€ç»Ÿè®¡
        if (response.success) {
            globalSuccessRequests.incrementAndGet();
            globalTotalInferenceTime.addAndGet((long) response.inferenceTimeMs);

            if (response.waitTimeMs > 0) {
                globalTotalWaitTime.addAndGet(response.waitTimeMs);
            }
            if (response.totalLatencyMs > 0) {
                globalTotalLatency.addAndGet(response.totalLatencyMs);
            } else {
                globalTotalLatency.addAndGet((long) response.inferenceTimeMs);
            }
        }

        // ç»Ÿè®¡GPUåˆ†å¸ƒ
        String gpuKey = response.modelName != null ? response.modelName : "Unknown-GPU";
        gpuRequestCounts.computeIfAbsent(gpuKey, k -> new AtomicInteger(0)).incrementAndGet();

        // ç»Ÿè®¡æ‰¹å¤§å°åˆ†å¸ƒ
        batchSizeDistribution.computeIfAbsent(response.batchSize, k -> new AtomicInteger(0)).incrementAndGet();

        // æ—¥å¿—è¾“å‡º
        if (localCount <= 3 || localCount % 25 == 0) {
            logger.info("å“åº” #{}: {} | GPU: {} | æ‰¹å¤§å°: {} | ç­‰å¾…: {}ms | {}",
                    globalCount, response.requestId, gpuKey, response.batchSize,
                    response.waitTimeMs, response.success ? "âœ…" : "âŒ");
        }

        // æ£€æŸ¥æ˜¯å¦è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ - ä¿®å¤ï¼šç¡®ä¿èƒ½æ­£ç¡®è§¦å‘ç»Ÿè®¡è¾“å‡º
        if (globalCount >= expectedTotalRequests && !globalFinalStatsOutputted) {
            logger.info("è§¦å‘æœ€ç»ˆç»Ÿè®¡: æ”¶åˆ°{}ä¸ªè¯·æ±‚ï¼Œé¢„æœŸ{}ä¸ª", globalCount, expectedTotalRequests);
            outputGlobalFinalStatsOnce();
        } else if (globalCount > expectedTotalRequests && !globalFinalStatsOutputted) {
            // é˜²æ­¢è¯·æ±‚æ•°è¶…è¿‡é¢„æœŸæ—¶é—æ¼ç»Ÿè®¡
            logger.warn("è¯·æ±‚æ•°{}è¶…è¿‡é¢„æœŸ{}ï¼Œå¼ºåˆ¶è¾“å‡ºç»Ÿè®¡", globalCount, expectedTotalRequests);
            outputGlobalFinalStatsOnce();
        }
    }

    private synchronized void outputGlobalFinalStatsOnce() {
        if (!globalFinalStatsOutputted) {
            globalFinalStatsOutputted = true;
            outputGlobalFinalStats();
        }
    }

    private void outputGlobalFinalStats() {
        int total = globalTotalRequests.get();
        int success = globalSuccessRequests.get();

        if (total == 0) {
            logger.warn("æ²¡æœ‰æ¥æ”¶åˆ°ä»»ä½•è¯·æ±‚");
            return;
        }

        // è®¡ç®—æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
        double actualProcessingTime;
        if (globalFirstResponseTime > 0 && globalLastResponseTime > globalFirstResponseTime) {
            actualProcessingTime = (globalLastResponseTime - globalFirstResponseTime) / 1000.0;
        } else {
            actualProcessingTime = (System.currentTimeMillis() - globalStartTime) / 1000.0;
        }

        double throughput = actualProcessingTime > 0 ? success / actualProcessingTime : 0.0;
        double avgLatency = success > 0 ? globalTotalLatency.get() / (double) success : 0.0;
        double avgWait = success > 0 ? globalTotalWaitTime.get() / (double) success : 0.0;
        double avgInference = success > 0 ? globalTotalInferenceTime.get() / (double) success : 0.0;
        double successRate = total > 0 ? (success * 100.0) / total : 0.0;

        // è®¡ç®—GPUè´Ÿè½½å‡è¡¡æŒ‡æ ‡
        double gpuLoadBalance = calculateGpuLoadBalance();
        int actualBatches = calculateActualBatches();
        double avgBatchSize = actualBatches > 0 ? (double) success / actualBatches : 0.0;

        // è®¡ç®—èµ„æºåˆ©ç”¨ç‡
        double theoreticalMaxThroughput = parallelism * (1000.0 / (300 + batchSize * 50)); // ç†è®ºæœ€å¤§ååé‡
        double resourceUtilization = theoreticalMaxThroughput > 0 ? (throughput / theoreticalMaxThroughput) * 100 : 0.0;

        logger.info("================================================");
        logger.info("=== pÃ—bè”åˆä¼˜åŒ–æœ€ç»ˆç»Ÿè®¡ ===");
        logger.info("================================================");
        logger.info("å®éªŒé…ç½®: p={}, b={} ({})", parallelism, batchSize, experimentId);
        logger.info("------------------------------------------------");
        logger.info("ğŸ”¢ è¯·æ±‚ç»Ÿè®¡:");
        logger.info("  æ€»è¯·æ±‚: {}", total);
        logger.info("  æˆåŠŸè¯·æ±‚: {}", success);
        logger.info("  æˆåŠŸç‡: {}%", String.format("%.1f", successRate));
        logger.info("------------------------------------------------");
        logger.info("âš¡ æ€§èƒ½æŒ‡æ ‡:");
        logger.info("  ååé‡: {} req/s", String.format("%.2f", throughput));
        logger.info("  å¹³å‡å»¶è¿Ÿ: {}ms", Math.round(avgLatency));
        logger.info("  å¹³å‡ç­‰å¾…: {}ms", Math.round(avgWait));
        logger.info("  å¹³å‡æ¨ç†: {}ms", Math.round(avgInference));
        logger.info("  å¤„ç†æ—¶é—´: {}s", String.format("%.1f", actualProcessingTime));
        logger.info("------------------------------------------------");
        logger.info("ğŸ”§ å¹¶è¡Œåº¦åˆ†æ:");
        logger.info("  å¹¶è¡Œåº¦(p): {}", parallelism);
        logger.info("  æ‰¹å¤§å°(b): {}", batchSize);
        logger.info("  å®é™…æ‰¹æ¬¡æ•°: {}", actualBatches);
        logger.info("  å¹³å‡æ‰¹å¤§å°: {}", String.format("%.1f", avgBatchSize));
        logger.info("  GPUè´Ÿè½½å‡è¡¡: {}%", String.format("%.1f", gpuLoadBalance));
        logger.info("  èµ„æºåˆ©ç”¨ç‡: {}%", String.format("%.1f", resourceUtilization));
        logger.info("------------------------------------------------");
        logger.info("ğŸ“Š GPUåˆ†å¸ƒ:");
        gpuRequestCounts.forEach((gpu, count) -> {
            double percentage = total > 0 ? (count.get() * 100.0) / total : 0.0;
            logger.info("  {}: {} è¯·æ±‚ ({}%)", gpu, count.get(), String.format("%.1f", percentage));
        });
        logger.info("------------------------------------------------");
        logger.info("ğŸ“¦ æ‰¹å¤§å°åˆ†å¸ƒ:");
        batchSizeDistribution.entrySet().stream()
                .sorted((e1, e2) -> e1.getKey().compareTo(e2.getKey()))
                .forEach(entry -> {
                    double percentage = total > 0 ? (entry.getValue().get() * 100.0) / total : 0.0;
                    logger.info("  æ‰¹å¤§å°{}: {} è¯·æ±‚ ({}%)",
                            entry.getKey(), entry.getValue().get(), String.format("%.1f", percentage));
                });
        logger.info("================================================");

        // å…³é”®æ€§èƒ½æ€»ç»“
        logger.info("ğŸ¯ å…³é”®æŒ‡æ ‡æ€»ç»“:");
        logger.info("  é…ç½®: p{}b{}", parallelism, batchSize);
        logger.info("  ååé‡: {} req/s", String.format("%.2f", throughput));
        logger.info("  å¹³å‡å»¶è¿Ÿ: {}ms", Math.round(avgLatency));
        logger.info("  GPUåˆ©ç”¨ç‡: {}%", String.format("%.1f", resourceUtilization));
        logger.info("================================================");
    }

    private double calculateGpuLoadBalance() {
        if (gpuRequestCounts.isEmpty()) return 0.0;

        int total = globalTotalRequests.get();
        double idealRequestsPerGpu = (double) total / parallelism;

        double variance = 0.0;
        for (AtomicInteger count : gpuRequestCounts.values()) {
            double diff = count.get() - idealRequestsPerGpu;
            variance += diff * diff;
        }
        variance /= gpuRequestCounts.size();

        double standardDeviation = Math.sqrt(variance);
        double coefficientOfVariation = idealRequestsPerGpu > 0 ? standardDeviation / idealRequestsPerGpu : 0.0;

        // è´Ÿè½½å‡è¡¡ç™¾åˆ†æ¯”ï¼šè¶Šæ¥è¿‘100%è¡¨ç¤ºè´Ÿè½½è¶Šå‡è¡¡
        return Math.max(0, Math.min(100, (1.0 - coefficientOfVariation) * 100));
    }

    private int calculateActualBatches() {
        // ä¼°ç®—å®é™…æ‰¹æ¬¡æ•°ï¼šå‡è®¾å¤§éƒ¨åˆ†è¯·æ±‚éƒ½åœ¨å®Œæ•´æ‰¹æ¬¡ä¸­
        int successRequests = globalSuccessRequests.get();
        return (int) Math.ceil((double) successRequests / batchSize);
    }
}