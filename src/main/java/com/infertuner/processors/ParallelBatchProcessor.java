package com.infertuner.processors;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.infertuner.models.InferenceRequest;
import com.infertuner.models.InferenceResponse;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.util.*;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentLinkedQueue;

/**
 * æ‰¹å¤„ç†å™¨ - ä¸ä½¿ç”¨Flink Stateï¼Œé¿å…keyByé—®é¢˜
 * ä½¿ç”¨å†…å­˜ç¼“å†²å®ç°æ”’æ‰¹åŠŸèƒ½
 */
public class ParallelBatchProcessor extends ProcessFunction<InferenceRequest, InferenceResponse> {

    private static final Logger logger = LoggerFactory.getLogger(ParallelBatchProcessor.class);

    // GPUç›¸å…³
    private int gpuId;
    private int taskIndex;
    private int totalParallelism;
    private transient Process inferenceProcess;  // æ ‡è®°ä¸ºtransient
    private transient BufferedWriter processInput;  // æ ‡è®°ä¸ºtransient
    private transient BufferedReader processOutput;  // æ ‡è®°ä¸ºtransient
    private transient ObjectMapper objectMapper;  // æ ‡è®°ä¸ºtransient

    // æ”’æ‰¹é…ç½®
    private int targetBatchSize = 4;
    private long maxWaitTimeMs = 3000;

    // å†…å­˜ç¼“å†²åŒº- åœ¨open()ä¸­åˆå§‹åŒ–
    private transient Queue<InferenceRequest> requestBuffer;
    private transient Queue<Long> arrivalTimes;
    private transient volatile long firstRequestTime = 0;
    private transient volatile int batchCounter = 0;
    private transient Object batchLock;

    private static final String MODEL_PATH = "/workspace/models/Qwen1.5-1.8B-Chat";
    private static final String BATCH_SERVICE_SCRIPT = "/workspace/infertuner/scripts/batch_inference_service.py";
    private static final int MAX_GPUS = 4;

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);

        // åˆå§‹åŒ–transientå­—æ®µ
        requestBuffer = new ConcurrentLinkedQueue<>();
        arrivalTimes = new ConcurrentLinkedQueue<>();
        batchLock = new Object();
        objectMapper = new ObjectMapper();

        taskIndex = getRuntimeContext().getIndexOfThisSubtask();
        totalParallelism = getRuntimeContext().getNumberOfParallelSubtasks();
        gpuId = taskIndex % MAX_GPUS;

        // ä»å…¨å±€å‚æ•°è·å–é…ç½®
        try {
            Map<String, String> globalParams = getRuntimeContext().getExecutionConfig()
                    .getGlobalJobParameters().toMap();

            if (globalParams.containsKey("batch.size")) {
                targetBatchSize = Integer.parseInt(globalParams.get("batch.size"));
            }
            if (globalParams.containsKey("max.wait.ms")) {
                maxWaitTimeMs = Long.parseLong(globalParams.get("max.wait.ms"));
            }
        } catch (Exception e) {
            logger.warn("ä½¿ç”¨é»˜è®¤é…ç½®: batchSize={}, maxWait={}ms", targetBatchSize, maxWaitTimeMs);
        }

        logger.info("ğŸ¯ GPU {} ç®€åŒ–å¹¶è¡Œæ”’æ‰¹å¤„ç†å™¨å¯åŠ¨: å¹¶è¡Œåº¦={}, æ‰¹å¤§å°={}, è¶…æ—¶={}ms",
                gpuId, totalParallelism, targetBatchSize, maxWaitTimeMs);
        logger.info("ğŸ“‹ GPU {} è´Ÿè´£å¤„ç†: taskIndex={}, ä½¿ç”¨å†…å­˜ç¼“å†²åŒº", gpuId, taskIndex);

        objectMapper.configure(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

        // å¯åŠ¨GPUæœåŠ¡
        startGPUService();

        logger.info("âœ… GPU {} ç®€åŒ–å¹¶è¡Œæ”’æ‰¹æœåŠ¡å¯åŠ¨å®Œæˆ", gpuId);
    }

    private void startGPUService() throws Exception {
        logger.info("å¯åŠ¨ GPU {} æ¨ç†æœåŠ¡...", gpuId);

        ProcessBuilder pb = new ProcessBuilder("python3", BATCH_SERVICE_SCRIPT, MODEL_PATH, String.valueOf(gpuId));
        pb.redirectErrorStream(false);
        inferenceProcess = pb.start();

        processInput = new BufferedWriter(new OutputStreamWriter(inferenceProcess.getOutputStream()));
        processOutput = new BufferedReader(new InputStreamReader(inferenceProcess.getInputStream()));

        // ç­‰å¾…æœåŠ¡å¯åŠ¨
        logger.info("ç­‰å¾… GPU {} æœåŠ¡å¯åŠ¨...", gpuId);
        Thread.sleep(3000);

        if (!inferenceProcess.isAlive()) {
            throw new RuntimeException("GPU " + gpuId + " æ¨ç†æœåŠ¡å¯åŠ¨å¤±è´¥");
        }

        logger.info("âœ… GPU {} æ¨ç†æœåŠ¡å¯åŠ¨æˆåŠŸ", gpuId);
    }

    @Override
    public void processElement(InferenceRequest request, Context ctx, Collector<InferenceResponse> out) throws Exception {
        long arrivalTime = request.timestamp;

        synchronized (batchLock) {
            // å°†è¯·æ±‚åŠ å…¥ç¼“å†²åŒº
            requestBuffer.offer(request);
            arrivalTimes.offer(arrivalTime);

            int currentSize = requestBuffer.size();

            // è®°å½•ç¬¬ä¸€ä¸ªè¯·æ±‚æ—¶é—´
            if (currentSize == 1) {
                firstRequestTime = arrivalTime;
                logger.info("GPU {} å¼€å§‹æ–°æ‰¹æ¬¡: {} (1/{}) - rebalanceåˆ†å‘", gpuId, request.requestId, targetBatchSize);
            }

            // ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ”’å¤Ÿäº† - å¦‚æœæ‰¹å¤§å°ä¸º1ï¼Œç«‹å³å¤„ç†
            if (currentSize >= targetBatchSize) {
                logger.info("ğŸš€ GPU {} æ”’å¤Ÿ{}ä¸ªè¯·æ±‚ï¼Œå¼€å§‹å¤„ç†", gpuId, targetBatchSize);
                processBatch(out, "æ•°é‡è§¦å‘");
            }
        }
    }

    private void processBatch(Collector<InferenceResponse> out, String triggerReason) throws Exception {
        List<InferenceRequest> batch = new ArrayList<>();
        List<Long> batchArrivalTimes = new ArrayList<>();

        // æå–æ‰¹æ¬¡è¯·æ±‚
        for (int i = 0; i < targetBatchSize && !requestBuffer.isEmpty(); i++) {
            InferenceRequest req = requestBuffer.poll();
            Long arrivalTime = arrivalTimes.poll();
            if (req != null && arrivalTime != null) {
                batch.add(req);
                batchArrivalTimes.add(arrivalTime);
            }
        }

        if (batch.isEmpty()) {
            return;
        }

        int batchSize = batch.size();
        int currentBatchNum = ++batchCounter;

        // ğŸ”§ ä¿®å¤æ‰¹æ¬¡è§¦å‘æ—¶é—´è®¡ç®—
        long realBatchTriggerTime;
        if ("æ•°é‡è§¦å‘".equals(triggerReason) && !batchArrivalTimes.isEmpty()) {
            // æ•°é‡è§¦å‘ï¼šä½¿ç”¨æœ€åä¸€ä¸ªè¯·æ±‚çš„åˆ°è¾¾æ—¶é—´ä½œä¸ºè§¦å‘æ—¶é—´
            realBatchTriggerTime = batchArrivalTimes.get(batchArrivalTimes.size() - 1);
            logger.info("ğŸ”¥ GPU {} æ‰¹æ¬¡#{} å¼€å§‹: {} | {}ä¸ªè¯·æ±‚ (è§¦å‘æ—¶é—´={})",
                    gpuId, currentBatchNum, triggerReason, batchSize,
                    new java.util.Date(realBatchTriggerTime));
        } else {
            // è¶…æ—¶è§¦å‘ï¼šä½¿ç”¨å½“å‰æ—¶é—´
            realBatchTriggerTime = System.currentTimeMillis();
            logger.info("ğŸ”¥ GPU {} æ‰¹æ¬¡#{} å¼€å§‹: {} | {}ä¸ªè¯·æ±‚ (è§¦å‘æ—¶é—´={})",
                    gpuId, currentBatchNum, triggerReason, batchSize,
                    new java.util.Date(realBatchTriggerTime));
        }

        // æ„å»ºæ‰¹é‡è¯·æ±‚
        BatchRequestData batchRequest = new BatchRequestData();
        batchRequest.requests = new ArrayList<>();

        for (InferenceRequest req : batch) {
            SingleRequestData singleReq = new SingleRequestData();
            singleReq.user_message = req.userMessage;
            singleReq.max_tokens = req.maxTokens;
            singleReq.request_id = req.requestId;
            batchRequest.requests.add(singleReq);
        }

        batchRequest.batch_size = batchSize;
        batchRequest.batch_id = String.format("gpu%d_batch_%d_%d", gpuId, currentBatchNum, realBatchTriggerTime);

        // å‘é€åˆ°GPUæœåŠ¡å¹¶è·å–å“åº”
        long batchStartTime = System.currentTimeMillis();

        String requestJson = objectMapper.writeValueAsString(batchRequest);
        processInput.write(requestJson + "\n");
        processInput.flush();

        String responseJson = processOutput.readLine();
        if (responseJson == null) {
            throw new RuntimeException("GPU " + gpuId + " æ— å“åº”");
        }

        BatchResponseData batchResponse = objectMapper.readValue(responseJson, BatchResponseData.class);

        if (!batchResponse.success) {
            throw new RuntimeException("GPU " + gpuId + " å¤„ç†å¤±è´¥: " + batchResponse.error);
        }

        long batchEndTime = System.currentTimeMillis();
        long totalProcessTime = batchEndTime - batchStartTime;
        double avgProcessTimePerRequest = (double) totalProcessTime / batchSize;

        logger.info("ğŸ“Š GPU {} æ‰¹æ¬¡#{} å®Œæˆ: æ€»æ—¶é—´={}ms, å¹³å‡={}ms/req",
                gpuId, currentBatchNum, totalProcessTime, String.format("%.1f", avgProcessTimePerRequest));

        // ç”Ÿæˆå“åº”å¹¶è¾“å‡º
        for (int i = 0; i < batch.size(); i++) {
            InferenceRequest originalReq = batch.get(i);
            SingleResponseData singleResp = batchResponse.responses.get(i);
            long requestArrivalTime = batchArrivalTimes.get(i);

            InferenceResponse response = new InferenceResponse();
            response.requestId = originalReq.requestId;
            response.userId = originalReq.userId;
            response.userMessage = originalReq.userMessage;
            response.aiResponse = singleResp.response;
            response.inferenceTimeMs = avgProcessTimePerRequest;
            response.success = singleResp.success;
            response.modelName = String.format("GPU-%d", gpuId);
            response.fromCache = false;
            response.batchSize = batchSize;
            response.timestamp = batchEndTime;

            // ğŸ”§ æ‰¹æ¬¡è§¦å‘æ—¶é—´è®¡ç®—ç­‰å¾…æ—¶é—´
            // ç­‰å¾…æ—¶é—´ = æ‰¹æ¬¡è§¦å‘æ—¶é—´ - è¯·æ±‚åˆ°è¾¾æ—¶é—´
            long waitTime = realBatchTriggerTime - requestArrivalTime;
            waitTime = Math.max(0, Math.min(waitTime, maxWaitTimeMs));

            response.waitTimeMs = waitTime;
            response.batchProcessTimeMs = totalProcessTime;
            response.totalLatencyMs = waitTime + (long)avgProcessTimePerRequest;

            // è¾“å‡ºå“åº”
            out.collect(response);
        }

        logger.info("âœ… GPU {} æ‰¹æ¬¡#{} è¾“å‡º{}ä¸ªå“åº”", gpuId, currentBatchNum, batchSize);
    }

    @Override
    public void close() throws Exception {
        logger.info("å…³é—­ GPU {} æœåŠ¡...", gpuId);

        try {
            if (processInput != null) {
                ShutdownCommand shutdownCmd = new ShutdownCommand();
                String shutdownJson = objectMapper.writeValueAsString(shutdownCmd);
                processInput.write(shutdownJson + "\n");
                processInput.flush();
                processInput.close();
            }

            if (processOutput != null) {
                processOutput.close();
            }

            if (inferenceProcess != null && inferenceProcess.isAlive()) {
                if (!inferenceProcess.waitFor(5, TimeUnit.SECONDS)) {
                    inferenceProcess.destroyForcibly();
                }
            }
        } catch (Exception e) {
            logger.error("å…³é—­GPUæœåŠ¡å‡ºé”™: {}", e.getMessage());
        }

        logger.info("âœ… GPU {} æœåŠ¡å·²å…³é—­", gpuId);
        super.close();
    }

    // æ•°æ®ç»“æ„ç±»
    private static class BatchRequestData {
        public List<SingleRequestData> requests;
        public int batch_size;
        public String batch_id;
    }

    private static class SingleRequestData {
        public String user_message;
        public int max_tokens;
        public String request_id;
    }

    private static class BatchResponseData {
        public List<SingleResponseData> responses;
        public int batch_size;
        public String batch_id;
        public long total_inference_time_ms;
        public boolean success;
        public String error;
        public long timestamp;
    }

    private static class SingleResponseData {
        public String response;
        public double inference_time_ms;
        public String request_id;
        public boolean success;
        public long timestamp;
    }

    private static class ShutdownCommand {
        public String command = "shutdown";
    }
}