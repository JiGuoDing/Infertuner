package com.infertuner.processors;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.infertuner.models.InferenceRequest;
import com.infertuner.models.InferenceResponse;
import org.apache.flink.api.common.state.ListState;
import org.apache.flink.api.common.state.ListStateDescriptor;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.UnknownHostException;
import java.util.*;
import java.util.concurrent.TimeUnit;

/**
 * æ‰¹å¤„ç†å™¨ - ä¸ä½¿ç”¨Flink Stateï¼Œé¿å…keyByé—®é¢˜
 * ä½¿ç”¨å†…å­˜ç¼“å†²å®ç°æ”’æ‰¹åŠŸèƒ½
 */
public class ParallelBatchProcessor extends ProcessFunction<InferenceRequest, InferenceResponse> {

    private static final Logger logger = LoggerFactory.getLogger(ParallelBatchProcessor.class);

    // GPUç›¸å…³
    private String nodeIP;
    private int taskIndex;
    private int totalParallelism;
    private transient Process inferenceProcess; // æ ‡è®°ä¸ºtransient
    private transient BufferedWriter processInput; // æ ‡è®°ä¸ºtransient
    private transient BufferedReader processOutput; // æ ‡è®°ä¸ºtransient
    private transient ObjectMapper objectMapper; // æ ‡è®°ä¸ºtransient
    private transient ListState<InferenceRequest> requestState; // Flink æ‰˜ç®¡çŠ¶æ€ï¼Œå‚¨å­˜è¯·æ±‚é˜Ÿåˆ—

    // æ”’æ‰¹é…ç½®
    private int inferenceBatchsize;
    private transient int batchCounter = 0;

    private static final String MODEL_NAME = "Falcon3-7B-Instruct";
    private static final String MODEL_PATH = "/mnt/tidal-alsh01/usr/suqian/models/".concat(MODEL_NAME);
    private static final String BATCH_SERVICE_SCRIPT = "/mnt/tidal-alsh01/usr/suqian/scripts/batch_inference_service_new.py";

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);

        // åˆå§‹åŒ–çŠ¶æ€å­˜å‚¨
        ListStateDescriptor<InferenceRequest> descriptor = new ListStateDescriptor<>("processed-request-buffer", InferenceRequest.class);
        requestState = getRuntimeContext().getListState(descriptor);

        objectMapper = new ObjectMapper();

        // è·å–çš„å½“å‰èŠ‚ç‚¹IP
        try {
            nodeIP = java.net.InetAddress.getLocalHost().getHostAddress();
        } catch (UnknownHostException e) {
            logger.error("è·å–å½“å‰èŠ‚ç‚¹IPå¤±è´¥", e);
            nodeIP = "Unknown-hostIP";
        }

        // ä»å…¨å±€å‚æ•°è·å– batchsize å‚æ•°
        try {
            Map<String, String> globalParams = getRuntimeContext().getExecutionConfig().getGlobalJobParameters()
                    .toMap();
            if (globalParams.containsKey("batchsize")) {
                inferenceBatchsize = Integer.parseInt(globalParams.get("batchsize"));
            }
        } catch (Exception ignored) {
        }

        objectMapper.configure(com.fasterxml.jackson.databind.DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

        // å¯åŠ¨å†…è”æ¨ç†æœåŠ¡è¿›ç¨‹
        startGPUService();

        logger.info("âœ… èŠ‚ç‚¹ {} æ”’æ‰¹æ¨ç†æœåŠ¡å¯åŠ¨å®Œæˆ", nodeIP);
    }

    private void startGPUService() throws Exception {
        logger.info("å¯åŠ¨èŠ‚ç‚¹ {} æ¨ç†æœåŠ¡...", nodeIP);

        ProcessBuilder pb = new ProcessBuilder(
                "/opt/conda/envs/vllm-env/bin/python", BATCH_SERVICE_SCRIPT, nodeIP, MODEL_PATH);
        pb.redirectErrorStream(false);
        pb.environment().put("PYTHONUNBUFFERED", "1");
        inferenceProcess = pb.start();

        processInput = new BufferedWriter(new OutputStreamWriter(inferenceProcess.getOutputStream()));
        processOutput = new BufferedReader(new InputStreamReader(inferenceProcess.getInputStream()));

        // ç­‰å¾…æœåŠ¡å¯åŠ¨
        logger.info("ç­‰å¾…èŠ‚ç‚¹ {} æœåŠ¡å¯åŠ¨...", nodeIP);
        Thread.sleep(5000);

        if (!inferenceProcess.isAlive()) {
            throw new RuntimeException("èŠ‚ç‚¹ " + nodeIP + " æ¨ç†æœåŠ¡å¯åŠ¨å¤±è´¥");
        }

        logger.info("âœ… èŠ‚ç‚¹ {} Python æ¨ç†è¿›ç¨‹å¯åŠ¨æˆåŠŸ", nodeIP);
    }

    @Override
    public void processElement(InferenceRequest request, Context ctx, Collector<InferenceResponse> out)
            throws Exception {
        // æ›´æ–°è¯·æ±‚çš„è¢«æ¥å—æ—¶é—´
        request.setAcceptedTimestamp(System.currentTimeMillis());

        List<InferenceRequest> currentRequestBatch = new ArrayList<>();
        for (InferenceRequest req : requestState.get()) {
            currentRequestBatch.add(req);
        }

        logger.info("èŠ‚ç‚¹ {} æ‰¹æ¬¡ {} æ”’æ‰¹è¿›åº¦ {}/{}", nodeIP, batchCounter, currentRequestBatch.size(), inferenceBatchsize);

        if (currentRequestBatch.size() >= inferenceBatchsize) {
            logger.info("ğŸš€ èŠ‚ç‚¹ {} æ”’å¤Ÿ {} ä¸ªè¯·æ±‚ï¼Œç¬¬ {} ä¸ªæ‰¹æ¬¡å¼€å§‹å¤„ç†", nodeIP, inferenceProcess, batchCounter);
            // æ¸…ç©ºçŠ¶æ€ï¼Œå‡†å¤‡æ¥æ”¶ä¸‹ä¸€æ‰¹è¯·æ±‚
            requestState.clear();

            for (InferenceRequest req : currentRequestBatch) {
                req.setProcessingTimestamp(System.currentTimeMillis());
            }

            // è¿›å…¥æ‰¹æ¬¡æ¨ç†æœåŠ¡
            long batchStartTime = System.currentTimeMillis();
            processBatch(out, currentRequestBatch);
            long batchEndTime = System.currentTimeMillis();

            logger.info("èŠ‚ç‚¹ {} ç¬¬ {} ä¸ªæ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè€—æ—¶ {}ms", nodeIP, batchCounter - 1, batchEndTime-batchStartTime);
        }
    }

    private void processBatch(Collector<InferenceResponse> out, List<InferenceRequest> requestBatch) throws Exception {
        if (requestBatch.isEmpty()) {
            return;
        }
        // å½“ä¸è€ƒè™‘è¶…æ—¶æ—¶ï¼Œæ‰¹æ¬¡è§¦å‘æ—¶é—´å³æœ€åä¸€ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´
        long batchTriggerTime = requestBatch.get(inferenceBatchsize - 1).getAcceptedTimestamp();

        List<RequestData> batchReqData = new ArrayList<>();
        for (InferenceRequest req : requestBatch) {
            // æ„é€ å•æ¡è¯·æ±‚ä½“
            RequestData reaData = new RequestData(req.getRequestId(), req.getUserId(), req.getUserMessage(), req.getLlmModel().getModelName());
            batchReqData.add(reaData);
        }

        // å°†è¯·æ±‚å‘é€åˆ° Python æ¨ç†è¿›ç¨‹
        String requestJson = objectMapper.writeValueAsString(batchReqData);
        processInput.write(requestJson + "\n");
        processInput.flush();

        // ä» Python æ¨ç†è¿›ç¨‹è·å–å“åº”
        String batchResponseJson = processOutput.readLine();
        if (batchResponseJson == null) {
            throw new RuntimeException("èŠ‚ç‚¹ " + nodeIP + " æ— å“åº”");
        }

        // æ„é€ å“åº”ä½“ï¼Œè§£æ Python æ¨ç†æœåŠ¡çš„å“åº”å†…å®¹
        String batchResponseString = processOutput.readLine();
        if (batchResponseString == null) {
            throw new RuntimeException("èŠ‚ç‚¹ " + nodeIP + " æ— å“åº”");
        }

        List<responseData> batchRespData = objectMapper.readValue(batchResponseString, new TypeReference<>() {});

        // ä»å“åº”ä¸­è·å–æ•°æ®
        for (int i = 0; i < requestBatch.size(); i++) {
            InferenceRequest originalRequest = requestBatch.get(i);
            responseData responseData = batchRespData.get(i);

            InferenceResponse response = new InferenceResponse();
            response.setRequestId(originalRequest.getRequestId());
            response.setUserId(originalRequest.getUserId());
            response.setUserMessage(originalRequest.getUserMessage());
            response.setResponseText(responseData.getResponse());
            response.setInferenceTimeMs(responseData.getInferenceTime());
            response.setSuccess(responseData.isSuccess());
            response.setNodeIP(nodeIP);
            response.setFromCache(false);
            response.setBatchSize(originalRequest.getBatchSize());
            response.setRequestAcceptedTime(originalRequest.getAcceptedTimestamp());
            response.setWaitTimeMs(batchTriggerTime - originalRequest.getAcceptedTimestamp());
            response.setTotalLatencyMs(response.getWaitTimeMs() + response.getInferenceTimeMs());

            logger.info("è¯·æ±‚ {} å¤„ç†å®Œæˆï¼Œç­‰å¾… {} æ¯«ç§’ï¼Œæ¨ç† {} æ¯«ç§’ï¼Œæ€»è€—æ—¶ {} æ¯«ç§’", response.requestId, response.waitTimeMs,
                    response.inferenceTimeMs, response.totalLatencyMs);
            // è¾“å‡ºå“åº”
            out.collect(response);
        }
    }

    @Override
    public void close() throws Exception {
        logger.info("å…³é—­èŠ‚ç‚¹ {} æœåŠ¡...", nodeIP);

        try {
            if (processInput != null) {
                ShutdownCommand shutdownCmd = new ShutdownCommand();
                String shutdownJson = objectMapper.writeValueAsString(shutdownCmd);
                processInput.write(shutdownJson + "\n");
                processInput.flush();
                processInput.close();
            }

            if (processOutput != null) {
                processOutput.close();
            }

            if (inferenceProcess != null && inferenceProcess.isAlive()) {
                if (!inferenceProcess.waitFor(5, TimeUnit.SECONDS)) {
                    inferenceProcess.destroyForcibly();
                }
            }
        } catch (Exception e) {
            logger.error("å…³é—­èŠ‚ç‚¹ -{} æœåŠ¡å‡ºé”™: {}", nodeIP, e.getMessage());
        }

        logger.info("âœ… èŠ‚ç‚¹ -{} æœåŠ¡å·²å…³é—­", nodeIP);
        super.close();
    }

    private static class RequestData {
        String requestId;
        String userId;
        String userMessage;
        String llmModelName;
        int batchsize;
        long predictedGeneratedTokenNum;
        double predictedInferenceTime;
        boolean success;

        RequestData() {};

        RequestData(String requestId, String userId, String userMessage, String llmModelName) {
            this.requestId = requestId;
            this.userId = userId;
            this.userMessage = userMessage;
            this.llmModelName = llmModelName;
            this.predictedGeneratedTokenNum = 0;
            this.predictedInferenceTime = 0.0;
            this.success = false;
        }


        RequestData(String requestId, String userId, String userMessage, String llmModelName, long predictedGeneratedTokenNum, double predictedInferenceTime, boolean success) {
            this.requestId = requestId;
            this.userId = userId;
            this.userMessage = userMessage;
            this.llmModelName = llmModelName;
            this.predictedGeneratedTokenNum = predictedGeneratedTokenNum;
            this.predictedInferenceTime = predictedInferenceTime;
            this.success = success;
        }

        public boolean isSuccess() {
            return success;
        }

        public void setSuccess(boolean success) {
            this.success = success;
        }

        public String getRequestId() {
            return requestId;
        }

        public void setRequestId(String requestId) {
            this.requestId = requestId;
        }

        public String getUserId() {
            return userId;
        }

        public void setUserId(String userId) {
            this.userId = userId;
        }

        public String getUserMessage() {
            return userMessage;
        }

        public void setUserMessage(String userMessage) {
            this.userMessage = userMessage;
        }

        public long getPredictedGeneratedTokenNum() {
            return predictedGeneratedTokenNum;
        }

        public void setPredictedGeneratedTokenNum(long predictedGeneratedTokenNum) {
            this.predictedGeneratedTokenNum = predictedGeneratedTokenNum;
        }

        public double getPredictedInferenceTime() {
            return predictedInferenceTime;
        }

        public void setPredictedInferenceTime(double predictedInferenceTime) {
            this.predictedInferenceTime = predictedInferenceTime;
        }

        public String getLlmModelName() {
            return llmModelName;
        }

        public void setLlmModelName(String llmModelName) {
            this.llmModelName = llmModelName;
        }

        public int getBatchsize() {
            return batchsize;
        }

        public void setBatchsize(int batchsize) {
            this.batchsize = batchsize;
        }
    }

    private static class responseData {
        String requestID;
        String response;
        boolean success;
        long inferenceTime;
        int generatedTokenNum;

        public responseData(String response, boolean success, long inferenceTime, int generatedTokenNum, String requestID) {
            this.response = response;
            this.success = success;
            this.inferenceTime = inferenceTime;
            this.generatedTokenNum = generatedTokenNum;
            this.requestID = requestID;
        }

        public boolean isSuccess() {
            return success;
        }

        public void setSuccess(boolean success) {
            this.success = success;
        }

        public String getResponse() {
            return response;
        }

        public void setResponse(String response) {
            this.response = response;
        }

        public String getRequestID() {
            return requestID;
        }

        public void setRequestID(String requestID) {
            this.requestID = requestID;
        }

        public int getGeneratedTokenNum() {
            return generatedTokenNum;
        }

        public void setGeneratedTokenNum(int generatedTokenNum) {
            this.generatedTokenNum = generatedTokenNum;
        }

        public long getInferenceTime() {
            return inferenceTime;
        }

        public void setInferenceTime(long inferenceTime) {
            this.inferenceTime = inferenceTime;
        }
    }

    private static class ShutdownCommand {
        public String command = "shutdown";
    }
}